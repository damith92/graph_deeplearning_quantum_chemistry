{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Fingerprint Solubility result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Task params {'target_name': 'measured log solubility in mols per litre', 'data_file': 'delaney.csv'}\n",
      "\n",
      "Starting neural fingerprint experiment...\n",
      "Total number of weights in the network: 68831\n",
      "max of weights 0.08886963478129936\n",
      "Iteration 0 loss 1.0719451738994792 train RMSE 2.2340124410642304 Validation RMSE 0 : 2.5261672692217907 max of weights 0.09101903063397289\n",
      "Iteration 10 loss 0.8723554254834599 train RMSE 1.817927278307013 Validation RMSE 10 : 1.9413312880371727 max of weights 0.11078477064514371\n",
      "Iteration 20 loss 0.8536294059961078 train RMSE 1.7787770464188364 Validation RMSE 20 : 1.9544333731754195 max of weights 0.13763792901196806\n",
      "Iteration 30 loss 0.7497380453642386 train RMSE 1.5620284153321324 Validation RMSE 30 : 1.7497666606754347 max of weights 0.16030121642932257\n",
      "Iteration 40 loss 0.589444734418833 train RMSE 1.2277007656195902 Validation RMSE 40 : 1.4871873276793541 max of weights 0.17854017192571806\n",
      "Iteration 50 loss 0.5316706205797443 train RMSE 1.107102754159304 Validation RMSE 50 : 1.4637046094617352 max of weights 0.20665857956349715\n",
      "Iteration 60 loss 0.5109749900994154 train RMSE 1.0638080848797602 Validation RMSE 60 : 1.4739695331466989 max of weights 0.23543466330055635\n",
      "Iteration 70 loss 0.4714012903462191 train RMSE 0.9811291316950964 Validation RMSE 70 : 1.3359526466289466 max of weights 0.27116099331261245\n",
      "Iteration 80 loss 0.4605912419809691 train RMSE 0.9584929768586976 Validation RMSE 80 : 1.1045218858486097 max of weights 0.29978504524489136\n",
      "Iteration 90 loss 0.39505678936461014 train RMSE 0.8218137330244017 Validation RMSE 90 : 1.034378076914516 max of weights 0.31866325967070397\n",
      "Iteration 100 loss 0.38511966847544327 train RMSE 0.8010029244702435 Validation RMSE 100 : 0.8962869378229626 max of weights 0.3278208053920119\n",
      "Iteration 110 loss 0.3648869720283376 train RMSE 0.7587672001154795 Validation RMSE 110 : 0.8314617008939197 max of weights 0.3358081438527471\n",
      "Iteration 120 loss 0.3583462887804851 train RMSE 0.7450695576736214 Validation RMSE 120 : 0.8183361055285421 max of weights 0.36693286059707236\n",
      "Iteration 130 loss 0.38060090145461506 train RMSE 0.7913958605449373 Validation RMSE 130 : 0.9554952408545702 max of weights 0.3872864295065555\n",
      "Iteration 140 loss 0.3599696387917234 train RMSE 0.748317876853668 Validation RMSE 140 : 0.9079765694611177 max of weights 0.4002800569771974\n",
      "Iteration 150 loss 0.3352322085347484 train RMSE 0.6967192598623064 Validation RMSE 150 : 0.8314735496558877 max of weights 0.4088507557268393\n",
      "Iteration 160 loss 0.3359500678183252 train RMSE 0.6981754957399794 Validation RMSE 160 : 0.7644634108811212 max of weights 0.43254009887539974\n",
      "Iteration 170 loss 0.3019498309091635 train RMSE 0.6272706094235309 Validation RMSE 170 : 0.8031947604459979 max of weights 0.4616376692685434\n",
      "Iteration 180 loss 0.3049145664796186 train RMSE 0.6334011681656432 Validation RMSE 180 : 0.8121924637509519 max of weights 0.4784197904641382\n",
      "Iteration 190 loss 0.29905681272328966 train RMSE 0.6211525839900036 Validation RMSE 190 : 0.7662752391822962 max of weights 0.4830565623793829\n",
      "Iteration 200 loss 0.2952019627360078 train RMSE 0.6130892564976639 Validation RMSE 200 : 0.7398176726110782 max of weights 0.48811897351520794\n",
      "Iteration 210 loss 0.28796227011100767 train RMSE 0.5979792835867586 Validation RMSE 210 : 0.73347248319661 max of weights 0.5001746915821407\n",
      "Iteration 220 loss 0.2831373284076354 train RMSE 0.5879006667673004 Validation RMSE 220 : 0.7007334788635585 max of weights 0.5127550882116205\n",
      "Iteration 230 loss 0.3044562843241069 train RMSE 0.6323090754267329 Validation RMSE 230 : 0.6905674322313352 max of weights 0.5198175236440522\n",
      "Iteration 240 loss 0.3152284045615122 train RMSE 0.6547164332375675 Validation RMSE 240 : 0.6899141004319808 max of weights 0.5275339388864277\n",
      "Iteration 250 loss 0.2925795836487027 train RMSE 0.6074674386507437 Validation RMSE 250 : 0.7076451731596825 max of weights 0.529579050375365\n",
      "Iteration 260 loss 0.29131875190102496 train RMSE 0.60480885205948 Validation RMSE 260 : 0.7413188229342643 max of weights 0.5268950022914656\n",
      "Iteration 270 loss 0.3062315832866439 train RMSE 0.6358615930304896 Validation RMSE 270 : 0.8284767758823401 max of weights 0.5333804690716208\n",
      "Iteration 280 loss 0.2989994372180949 train RMSE 0.6207537063467887 Validation RMSE 280 : 0.7652472215387172 max of weights 0.5417326714324986\n",
      "Iteration 290 loss 0.2882232512000151 train RMSE 0.5982589459875455 Validation RMSE 290 : 0.7595033167964742 max of weights 0.5496701068084907\n",
      "Iteration 300 loss 0.3085396289075043 train RMSE 0.6405575080282383 Validation RMSE 300 : 0.8264452768989781 max of weights 0.5498272174610441\n",
      "Iteration 310 loss 0.3024728905120333 train RMSE 0.6278869293560786 Validation RMSE 310 : 0.8471815840467535 max of weights 0.554290905335766\n",
      "Iteration 320 loss 0.3007484213951263 train RMSE 0.6242672125761047 Validation RMSE 320 : 0.7371802231204894 max of weights 0.5632330719148744\n",
      "Iteration 330 loss 0.27158962105554196 train RMSE 0.5634596309130968 Validation RMSE 330 : 0.6977919450168488 max of weights 0.5714753249876531\n",
      "Iteration 340 loss 0.27064456291841354 train RMSE 0.5614665276934185 Validation RMSE 340 : 0.6985948234583469 max of weights 0.5824654414668561\n",
      "Iteration 350 loss 0.285267090643849 train RMSE 0.5919270935825262 Validation RMSE 350 : 0.6842350990126489 max of weights 0.587708449607173\n",
      "Iteration 360 loss 0.2942240477264791 train RMSE 0.6105632754866823 Validation RMSE 360 : 0.6754171929118722 max of weights 0.5891181603634652\n",
      "Iteration 370 loss 0.27497414575129825 train RMSE 0.5704028337286214 Validation RMSE 370 : 0.6550843482831046 max of weights 0.5992315682777565\n",
      "Iteration 380 loss 0.2773906978175243 train RMSE 0.5754206136591999 Validation RMSE 380 : 0.7472505984023219 max of weights 0.6087689157752474\n",
      "Iteration 390 loss 0.2892438019117575 train RMSE 0.6000956419140887 Validation RMSE 390 : 0.7847394531501576 max of weights 0.6146459353423407\n",
      "Iteration 400 loss 0.2813520014286813 train RMSE 0.5836090538081643 Validation RMSE 400 : 0.7452267268263583 max of weights 0.6256210660964382\n",
      "Iteration 410 loss 0.27349469029906076 train RMSE 0.5672031961994073 Validation RMSE 410 : 0.725920132647241 max of weights 0.6303420417647685\n",
      "Iteration 420 loss 0.29972222511251656 train RMSE 0.6218255164746969 Validation RMSE 420 : 0.7280247306726582 max of weights 0.6326784250477693\n",
      "Iteration 430 loss 0.28742653513258315 train RMSE 0.5961687774873808 Validation RMSE 430 : 0.7655750793905624 max of weights 0.6399750487753156\n",
      "Iteration 440 loss 0.28100979444080276 train RMSE 0.5827615199890425 Validation RMSE 440 : 0.7180624391900634 max of weights 0.6525527930729661\n",
      "Iteration 450 loss 0.2557128845593372 train RMSE 0.5300083286655415 Validation RMSE 450 : 0.7051374345571941 max of weights 0.6600950836508468\n",
      "Iteration 460 loss 0.25524405447613613 train RMSE 0.5290045761221963 Validation RMSE 460 : 0.682410949122328 max of weights 0.6672624172484676\n",
      "Iteration 470 loss 0.2762252739901182 train RMSE 0.5727153027611414 Validation RMSE 470 : 0.6724337529360037 max of weights 0.6690754179823983\n",
      "Iteration 480 loss 0.2811837260467771 train RMSE 0.5830193151069963 Validation RMSE 480 : 0.6555755597024786 max of weights 0.6747343017650438\n",
      "Iteration 490 loss 0.26059382091405103 train RMSE 0.5400651489123149 Validation RMSE 490 : 0.5969518380245484 max of weights 0.6800816246640776\n",
      "Iteration 500 loss 0.2647273999655886 train RMSE 0.548660894598989 Validation RMSE 500 : 0.6543951383980896 max of weights 0.692689258517486\n",
      "Iteration 510 loss 0.26061578200781726 train RMSE 0.5400762464320708 Validation RMSE 510 : 0.6829506855122749 max of weights 0.7088675929730086\n",
      "Iteration 520 loss 0.2686235286670386 train RMSE 0.556738336610236 Validation RMSE 520 : 0.6858540239362596 max of weights 0.7185101329767419\n",
      "Iteration 530 loss 0.2577609027901239 train RMSE 0.5340709870347679 Validation RMSE 530 : 0.7060731146095113 max of weights 0.7274299044943328\n",
      "Iteration 540 loss 0.2735977814993159 train RMSE 0.5670475225411097 Validation RMSE 540 : 0.7347211014545577 max of weights 0.7310086539483388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 550 loss 0.29048458528884497 train RMSE 0.6022145996638145 Validation RMSE 550 : 0.7869607408011478 max of weights 0.7370442888662246\n",
      "Iteration 560 loss 0.2842842032458141 train RMSE 0.5892681631196803 Validation RMSE 560 : 0.756760267204342 max of weights 0.7484396999247419\n",
      "Iteration 570 loss 0.24843683568301336 train RMSE 0.5145227415313163 Validation RMSE 570 : 0.6996722674714357 max of weights 0.756093627471169\n",
      "Iteration 580 loss 0.26534273040413947 train RMSE 0.5497345633975582 Validation RMSE 580 : 0.6622284480872511 max of weights 0.7610512795845409\n",
      "Iteration 590 loss 0.2751201676693799 train RMSE 0.5700865865318614 Validation RMSE 590 : 0.6785113162181864 max of weights 0.7630715524633231\n",
      "Iteration 600 loss 0.2702422910863891 train RMSE 0.5598863531263204 Validation RMSE 600 : 0.6634249659431887 max of weights 0.7714029022037596\n",
      "Iteration 610 loss 0.24695178034687978 train RMSE 0.5113213920227726 Validation RMSE 610 : 0.6290468116358046 max of weights 0.7795801051765497\n",
      "Iteration 620 loss 0.2481723783819863 train RMSE 0.513843242787542 Validation RMSE 620 : 0.6514124907441893 max of weights 0.7889215712803573\n",
      "Iteration 630 loss 0.26629976784003867 train RMSE 0.5516029852393856 Validation RMSE 630 : 0.6489605060781056 max of weights 0.7910909410735687\n",
      "Iteration 640 loss 0.27500712843538544 train RMSE 0.5697060325451945 Validation RMSE 640 : 0.6289525454366206 max of weights 0.7879127849728493\n",
      "Iteration 650 loss 0.25996761287570047 train RMSE 0.538330469348573 Validation RMSE 650 : 0.6095881596624746 max of weights 0.7887855190439206\n",
      "Iteration 660 loss 0.25645534702691997 train RMSE 0.5309983580996094 Validation RMSE 660 : 0.6086191629327915 max of weights 0.7940843836968946\n",
      "Iteration 670 loss 0.2655890006066414 train RMSE 0.5500301008886872 Validation RMSE 670 : 0.6670743448580054 max of weights 0.8019971852569263\n",
      "Iteration 680 loss 0.2728786610012609 train RMSE 0.5652043140742425 Validation RMSE 680 : 0.6321450931483114 max of weights 0.8064904690901831\n",
      "Iteration 690 loss 0.25547556626264567 train RMSE 0.5289022055136243 Validation RMSE 690 : 0.6395257935006377 max of weights 0.8012942330596605\n",
      "Iteration 700 loss 0.26585074535832665 train RMSE 0.5504972641004926 Validation RMSE 700 : 0.6806114235566397 max of weights 0.8044865460428303\n",
      "Iteration 710 loss 0.2552658723040484 train RMSE 0.5284265242211313 Validation RMSE 710 : 0.6391365657302438 max of weights 0.8116088669495475\n",
      "Iteration 720 loss 0.2742106369288647 train RMSE 0.567890603679231 Validation RMSE 720 : 0.6819732855072171 max of weights 0.8146369874078639\n",
      "Iteration 730 loss 0.2454228464421233 train RMSE 0.5078534485252898 Validation RMSE 730 : 0.6541507972100771 max of weights 0.8106360421200492\n",
      "Iteration 740 loss 0.252293558731458 train RMSE 0.5221475113224201 Validation RMSE 740 : 0.6741844051026272\n",
      "Neural test RMSE: 0.5040404769114979\n"
     ]
    }
   ],
   "source": [
    "# Example regression script using neural fingerprints.\n",
    "#\n",
    "# Compares Morgan fingerprints to neural fingerprints.\n",
    "\n",
    "import autograd.numpy as np\n",
    "import autograd.numpy.random as npr\n",
    "\n",
    "from neuralfingerprint import load_data\n",
    "from neuralfingerprint import build_morgan_deep_net\n",
    "from neuralfingerprint import build_conv_deep_net\n",
    "from neuralfingerprint import normalize_array, adam\n",
    "from neuralfingerprint import build_batched_grad\n",
    "from neuralfingerprint.util import rmse\n",
    "\n",
    "from autograd import grad\n",
    "\n",
    "task_params = {'target_name' : 'measured log solubility in mols per litre',\n",
    "               'data_file'   : 'delaney.csv'}\n",
    "N_train = 800\n",
    "N_val   = 20\n",
    "N_test  = 20\n",
    "\n",
    "model_params = dict(fp_length=50,    # Usually neural fps need far fewer dimensions than morgan.\n",
    "                    fp_depth=6,      # The depth of the network equals the fingerprint radius.\n",
    "                    conv_width=30,   # Only the neural fps need this parameter.\n",
    "                    h1_size=100,     # Size of hidden layer of network on top of fps.\n",
    "                    L2_reg=np.exp(-2))\n",
    "train_params = dict(num_iters=750,\n",
    "                    batch_size=100,\n",
    "                    init_scale=np.exp(-4),\n",
    "                    step_size=np.exp(-6))\n",
    "\n",
    "# Define the architecture of the network that sits on top of the fingerprints.\n",
    "vanilla_net_params = dict(\n",
    "    layer_sizes = [model_params['fp_length'], model_params['h1_size']],  # One hidden layer.\n",
    "    normalize=True, L2_reg = model_params['L2_reg'], nll_func = rmse)\n",
    "\n",
    "def train_nn(pred_fun, loss_fun, num_weights, train_smiles, train_raw_targets, train_params, seed=0,\n",
    "             validation_smiles=None, validation_raw_targets=None):\n",
    "    \"\"\"loss_fun has inputs (weights, smiles, targets)\"\"\"\n",
    "    print \"Total number of weights in the network:\", num_weights\n",
    "    init_weights = npr.RandomState(seed).randn(num_weights) * train_params['init_scale']\n",
    "\n",
    "    num_print_examples = 100\n",
    "    train_targets, undo_norm = normalize_array(train_raw_targets)\n",
    "    training_curve = []\n",
    "    def callback(weights, iter):\n",
    "        if iter % 10 == 0:\n",
    "            print \"max of weights\", np.max(np.abs(weights))\n",
    "            train_preds = undo_norm(pred_fun(weights, train_smiles[:num_print_examples]))\n",
    "            cur_loss = loss_fun(weights, train_smiles[:num_print_examples], train_targets[:num_print_examples])\n",
    "            training_curve.append(cur_loss)\n",
    "            print \"Iteration\", iter, \"loss\", cur_loss,\\\n",
    "                  \"train RMSE\", rmse(train_preds, train_raw_targets[:num_print_examples]),\n",
    "            if validation_smiles is not None:\n",
    "                validation_preds = undo_norm(pred_fun(weights, validation_smiles))\n",
    "                print \"Validation RMSE\", iter, \":\", rmse(validation_preds, validation_raw_targets),\n",
    "\n",
    "    # Build gradient using autograd.\n",
    "    grad_fun = grad(loss_fun)\n",
    "    grad_fun_with_data = build_batched_grad(grad_fun, train_params['batch_size'],\n",
    "                                            train_smiles, train_targets)\n",
    "\n",
    "    # Optimize weights.\n",
    "    trained_weights = adam(grad_fun_with_data, init_weights, callback=callback,\n",
    "                           num_iters=train_params['num_iters'], step_size=train_params['step_size'])\n",
    "\n",
    "    def predict_func(new_smiles):\n",
    "        \"\"\"Returns to the original units that the raw targets were in.\"\"\"\n",
    "        return undo_norm(pred_fun(trained_weights, new_smiles))\n",
    "    return predict_func, trained_weights, training_curve\n",
    "\n",
    "\n",
    "def main():\n",
    "    print \"Loading data...\"\n",
    "    traindata, valdata, testdata = load_data(\n",
    "        task_params['data_file'], (N_train, N_val, N_test),\n",
    "        input_name='smiles', target_name=task_params['target_name'])\n",
    "    train_inputs, train_targets = traindata\n",
    "    val_inputs,   val_targets   = valdata\n",
    "    test_inputs,  test_targets  = testdata\n",
    "\n",
    "    def print_performance(pred_func):\n",
    "        train_preds = pred_func(train_inputs)\n",
    "        val_preds = pred_func(val_inputs)\n",
    "        print \"\\nPerformance (RMSE) on \" + task_params['target_name'] + \":\"\n",
    "        print \"Train:\", rmse(train_preds, train_targets)\n",
    "        print \"Test: \", rmse(val_preds,  val_targets)\n",
    "        print \"-\" * 80\n",
    "        return rmse(val_preds, val_targets)\n",
    "\n",
    "    def run_morgan_experiment():\n",
    "        loss_fun, pred_fun, net_parser = \\\n",
    "            build_morgan_deep_net(model_params['fp_length'],\n",
    "                                  model_params['fp_depth'], vanilla_net_params)\n",
    "        num_weights = len(net_parser)\n",
    "        predict_func, trained_weights, conv_training_curve = \\\n",
    "            train_nn(pred_fun, loss_fun, num_weights, train_inputs, train_targets,\n",
    "                     train_params, validation_smiles=val_inputs, validation_raw_targets=val_targets)\n",
    "        return print_performance(predict_func)\n",
    "\n",
    "    def run_conv_experiment():\n",
    "        conv_layer_sizes = [model_params['conv_width']] * model_params['fp_depth']\n",
    "        conv_arch_params = {'num_hidden_features' : conv_layer_sizes,\n",
    "                            'fp_length' : model_params['fp_length'], 'normalize' : 1}\n",
    "        loss_fun, pred_fun, conv_parser = \\\n",
    "            build_conv_deep_net(conv_arch_params, vanilla_net_params, model_params['L2_reg'])\n",
    "        num_weights = len(conv_parser)\n",
    "        predict_func, trained_weights, conv_training_curve = \\\n",
    "            train_nn(pred_fun, loss_fun, num_weights, train_inputs, train_targets,\n",
    "                     train_params, validation_smiles=val_inputs, validation_raw_targets=val_targets)\n",
    "        test_predictions = predict_func(test_inputs)\n",
    "        return rmse(test_predictions, test_targets)\n",
    "\n",
    "    print \"Task params\", task_params\n",
    "    print\n",
    "    #print \"Starting Morgan fingerprint experiment...\"\n",
    "    #test_loss_morgan = run_morgan_experiment()\n",
    "    print \"Starting neural fingerprint experiment...\"\n",
    "    test_loss_neural = run_conv_experiment()\n",
    "    print\n",
    "    #print \"Morgan test RMSE:\", test_loss_morgan, \"Neural test RMSE:\", test_loss_neural\n",
    "    print \"Neural test RMSE:\", test_loss_neural\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Fingerprint photovoltaic efficiency result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Task params {'target_name': 'PCE', 'data_file': 'cep-processed.csv'}\n",
      "\n",
      "Starting neural fingerprint experiment...\n",
      "Total number of weights in the network: 102891\n",
      "max of weights 0.08886963478129936\n",
      "Iteration 0 loss 0.9409373354488176 train RMSE 2.382422934103587 Validation RMSE 0 : 2.3523152794061186 max of weights 0.08590965845178118\n",
      "Iteration 10 loss 0.9321649813003298 train RMSE 2.3602039514325552 Validation RMSE 10 : 2.1881559058617785 max of weights 0.10318460249667184\n",
      "Iteration 20 loss 0.9187027156022843 train RMSE 2.3260769141817734 Validation RMSE 20 : 2.1907101090913486 max of weights 0.1294722529595563\n",
      "Iteration 30 loss 0.7927960732696413 train RMSE 2.006955776490843 Validation RMSE 30 : 1.6588675756833113 max of weights 0.14045331413412346\n",
      "Iteration 40 loss 0.7339702671477818 train RMSE 1.8577070679457697 Validation RMSE 40 : 1.4163022716488234 max of weights 0.1651769062531167\n",
      "Iteration 50 loss 0.7090922877865006 train RMSE 1.7947655659496542 Validation RMSE 50 : 1.5785624492824606 max of weights 0.1993118684458645\n",
      "Iteration 60 loss 0.6660248615323405 train RMSE 1.6856004331559644 Validation RMSE 60 : 1.3762202057014845 max of weights 0.23355466326412275\n",
      "Iteration 70 loss 0.6946052378192166 train RMSE 1.7578674189149979 Validation RMSE 70 : 1.282411432357967 max of weights 0.26261384895296797\n",
      "Iteration 80 loss 0.669362805669147 train RMSE 1.693931251803849 Validation RMSE 80 : 1.3261612671534808 max of weights 0.2896095099833493\n",
      "Iteration 90 loss 0.6339491802142235 train RMSE 1.6042059485756948 Validation RMSE 90 : 1.3272933826595112 max of weights 0.30923046287376565\n",
      "Iteration 100 loss 0.6513661833400718 train RMSE 1.6482411513452966 Validation RMSE 100 : 1.2530711976546332 max of weights 0.32975712810019925\n",
      "Iteration 110 loss 0.6543504844507051 train RMSE 1.6557951265672501 Validation RMSE 110 : 1.3140594609845664 max of weights 0.35054694653661267\n",
      "Iteration 120 loss 0.6587343295205915 train RMSE 1.6668420373806245 Validation RMSE 120 : 1.3506817514232474 max of weights 0.3702573423980657\n",
      "Iteration 130 loss 0.6425825016189853 train RMSE 1.625897233358286 Validation RMSE 130 : 1.305761288856655 max of weights 0.3836137364659463\n",
      "Iteration 140 loss 0.6623885739874187 train RMSE 1.676024963128218 Validation RMSE 140 : 1.3620015248413646 max of weights 0.40463977935083345\n",
      "Iteration 150 loss 0.640655836698967 train RMSE 1.6209789774325563 Validation RMSE 150 : 1.3004722233588644 max of weights 0.43365995665987483\n",
      "Iteration 160 loss 0.6872854762120529 train RMSE 1.7389758638809902 Validation RMSE 160 : 1.3261595328219526 max of weights 0.45411590031375254\n",
      "Iteration 170 loss 0.6940725471628302 train RMSE 1.7561762401639085 Validation RMSE 170 : 1.4485945040782986 max of weights 0.4615560189497011\n",
      "Iteration 180 loss 0.6323834290641155 train RMSE 1.5999402718801543 Validation RMSE 180 : 1.3316670390520995 max of weights 0.4589094129642617\n",
      "Iteration 190 loss 0.6410123825026289 train RMSE 1.6218142775324187 Validation RMSE 190 : 1.3199122977837554 max of weights 0.4691840609920542\n",
      "Iteration 200 loss 0.6278382367803012 train RMSE 1.5884230086896571 Validation RMSE 200 : 1.3382868228510432 max of weights 0.4763186594999796\n",
      "Iteration 210 loss 0.6419529030358508 train RMSE 1.624129714164972 Validation RMSE 210 : 1.3587326316486525 max of weights 0.4871014294506009\n",
      "Iteration 220 loss 0.6370824302275955 train RMSE 1.6117613903266121 Validation RMSE 220 : 1.346406307914015 max of weights 0.4899638179996866\n",
      "Iteration 230 loss 0.6345074488144866 train RMSE 1.605175774182726 Validation RMSE 230 : 1.2656892512425437 max of weights 0.49454808271006623\n",
      "Iteration 240 loss 0.62135153224432 train RMSE 1.5718746940895565 Validation RMSE 240 : 1.3419249832964466 max of weights 0.4955545114667102\n",
      "Iteration 250 loss 0.6319069799933041 train RMSE 1.5985715319731533 Validation RMSE 250 : 1.3950047151598184 max of weights 0.49150587069025536\n",
      "Iteration 260 loss 0.6453232096703574 train RMSE 1.6325178900250914 Validation RMSE 260 : 1.3610181740723335 max of weights 0.49840583725299853\n",
      "Iteration 270 loss 0.6509351427171519 train RMSE 1.646688335707814 Validation RMSE 270 : 1.3255307720022682 max of weights 0.5048483929575194\n",
      "Iteration 280 loss 0.6299778496502411 train RMSE 1.593609715410626 Validation RMSE 280 : 1.3010193380227124 max of weights 0.5189476983132335\n",
      "Iteration 290 loss 0.6331099491370428 train RMSE 1.601521329210424 Validation RMSE 290 : 1.3809852456994813 max of weights 0.52541287488591\n",
      "Iteration 300 loss 0.6564590597620802 train RMSE 1.660589098401381 Validation RMSE 300 : 1.2469283137088065 max of weights 0.5425283821149826\n",
      "Iteration 310 loss 0.6410144134536971 train RMSE 1.621484481003549 Validation RMSE 310 : 1.3703272312935237 max of weights 0.559000790858849\n",
      "Iteration 320 loss 0.6701380229801295 train RMSE 1.6951705784147937 Validation RMSE 320 : 1.3743718978216857 max of weights 0.5710550010975793\n",
      "Iteration 330 loss 0.6370997032968782 train RMSE 1.6114868396990625 Validation RMSE 330 : 1.3018696014972253 max of weights 0.5784715061577265\n",
      "Iteration 340 loss 0.659138480437016 train RMSE 1.667261233150849 Validation RMSE 340 : 1.3950044775883934 max of weights 0.5833761799996132\n",
      "Iteration 350 loss 0.6345035642391084 train RMSE 1.6048942863170461 Validation RMSE 350 : 1.354794623365317 max of weights 0.5926805400188083\n",
      "Iteration 360 loss 0.70912467803522 train RMSE 1.793778875908026 Validation RMSE 360 : 1.3806197706423093 max of weights 0.6039735698818723\n",
      "Iteration 370 loss 0.6541382556979533 train RMSE 1.6545732473857173 Validation RMSE 370 : 1.3866051856921444 max of weights 0.6089617539359538\n",
      "Iteration 380 loss 0.626551287723103 train RMSE 1.5846987334696858 Validation RMSE 380 : 1.344105709275403 max of weights 0.6143051053936928\n",
      "Iteration 390 loss 0.6323181004718555 train RMSE 1.599344658972997 Validation RMSE 390 : 1.3745027333607494 max of weights 0.6299345162919733\n",
      "Iteration 400 loss 0.6134671603352257 train RMSE 1.551558819546399 Validation RMSE 400 : 1.3309067565127375 max of weights 0.6436355982171222\n",
      "Iteration 410 loss 0.6318499672621882 train RMSE 1.5980835841011105 Validation RMSE 410 : 1.3867396352870585 max of weights 0.653600235326459\n",
      "Iteration 420 loss 0.6328682212797299 train RMSE 1.6006144915779708 Validation RMSE 420 : 1.3593795011409666 max of weights 0.6555837937247472\n",
      "Iteration 430 loss 0.6268050678289091 train RMSE 1.5852171207507784 Validation RMSE 430 : 1.25120865769471 max of weights 0.6646392472078172\n",
      "Iteration 440 loss 0.5984505073633604 train RMSE 1.5134195624020765 Validation RMSE 440 : 1.346865487870155 max of weights 0.6768354344058869\n",
      "Iteration 450 loss 0.6180581387357781 train RMSE 1.5630517340818224 Validation RMSE 450 : 1.3867658694548934 max of weights 0.6913180383736197\n",
      "Iteration 460 loss 0.6354042815699225 train RMSE 1.6069553330280222 Validation RMSE 460 : 1.417470810090525 max of weights 0.7011893659914938\n",
      "Iteration 470 loss 0.6307959473810858 train RMSE 1.595236414316483 Validation RMSE 470 : 1.3230965822420637 max of weights 0.708441847929123\n",
      "Iteration 480 loss 0.6116076794102853 train RMSE 1.5466182463483298 Validation RMSE 480 : 1.307831173553675 max of weights 0.7232751012173074\n",
      "Iteration 490 loss 0.6332923316225112 train RMSE 1.6014931336691347 Validation RMSE 490 : 1.3961171632091731 max of weights 0.7273504517305225\n",
      "Iteration 500 loss 0.6436285833939446 train RMSE 1.6275971253723274 Validation RMSE 500 : 1.2601914770815312 max of weights 0.7379073696252555\n",
      "Iteration 510 loss 0.6322547400483977 train RMSE 1.5987792503024008 Validation RMSE 510 : 1.4372688476146203 max of weights 0.7462902052334408\n",
      "Iteration 520 loss 0.6398356504493984 train RMSE 1.6179131018206532 Validation RMSE 520 : 1.382109845889386 max of weights 0.7485941659295279\n",
      "Iteration 530 loss 0.6320904506052962 train RMSE 1.5982588116924237 Validation RMSE 530 : 1.3790812430078507 max of weights 0.7510433601137659\n",
      "Iteration 540 loss 0.6266484854631301 train RMSE 1.5844359936713832 Validation RMSE 540 : 1.354536272278428 max of weights 0.7528731435784152\n",
      "Iteration 550 loss 0.6204756159521515 train RMSE 1.5687935522642418 Validation RMSE 550 : 1.424830273496736 max of weights 0.7532922495019025\n",
      "Iteration 560 loss 0.6696066376381858 train RMSE 1.693153627617034 Validation RMSE 560 : 1.3526895886794952 max of weights 0.759699397389538\n",
      "Iteration 570 loss 0.6112812285453532 train RMSE 1.5454398243539844 Validation RMSE 570 : 1.348565119780223 max of weights 0.760474426816135\n",
      "Iteration 580 loss 0.593615141918929 train RMSE 1.50068365320079 Validation RMSE 580 : 1.318987905395142 max of weights 0.765073832501627\n",
      "Iteration 590 loss 0.599418103784649 train RMSE 1.5153891312548513 Validation RMSE 590 : 1.36573528585528 max of weights 0.7744342761935007\n",
      "Iteration 600 loss 0.5935798234119491 train RMSE 1.500539512790678 Validation RMSE 600 : 1.2958890318975052 max of weights 0.784918085533305\n",
      "Iteration 610 loss 0.6040895555277922 train RMSE 1.5271410055880281 Validation RMSE 610 : 1.3529781516836 max of weights 0.7946345924874939\n",
      "Iteration 620 loss 0.6263220186038472 train RMSE 1.5833903863451861 Validation RMSE 620 : 1.3949146641299008 max of weights 0.794480723499204\n",
      "Iteration 630 loss 0.5718421599795078 train RMSE 1.4453838998228294 Validation RMSE 630 : 1.2887311929761458 max of weights 0.8028650049006476\n",
      "Iteration 640 loss 0.5717103756704882 train RMSE 1.445018551683621 Validation RMSE 640 : 1.3834330818430658 max of weights 0.8075922633694842\n",
      "Iteration 650 loss 0.5619059404677803 train RMSE 1.420133773496509 Validation RMSE 650 : 1.3455144570594668 max of weights 0.8146691986679864\n",
      "Iteration 660 loss 0.6049174043532065 train RMSE 1.5290234979160726 Validation RMSE 660 : 1.4359737492800533 max of weights 0.821756565625775\n",
      "Iteration 670 loss 0.5824165013929623 train RMSE 1.4720151984976377 Validation RMSE 670 : 1.3252166838434285 max of weights 0.8266273708920583\n",
      "Iteration 680 loss 0.5821093573217642 train RMSE 1.4711961880897693 Validation RMSE 680 : 1.3959319593229478 max of weights 0.8255109513013219\n",
      "Iteration 690 loss 0.5799113967328666 train RMSE 1.4655723834107381 Validation RMSE 690 : 1.340531244915801 max of weights 0.8271828762335922\n",
      "Iteration 700 loss 0.5761490258251886 train RMSE 1.455985543710059 Validation RMSE 700 : 1.2905916752466573 max of weights 0.8401556326502156\n",
      "Iteration 710 loss 0.5848211922693352 train RMSE 1.4779082381213497 Validation RMSE 710 : 1.395092430035943 max of weights 0.8501635122058682\n",
      "Iteration 720 loss 0.5895461202035002 train RMSE 1.489803295633302 Validation RMSE 720 : 1.304491105612049 max of weights 0.8475650051600723\n",
      "Iteration 730 loss 0.5860186035061684 train RMSE 1.4808446312300743 Validation RMSE 730 : 1.4328174341047275 max of weights 0.8512039508257296\n",
      "Iteration 740 loss 0.5614198931579565 train RMSE 1.418480599134329 Validation RMSE 740 : 1.2952826005646292 max of weights 0.8587833036012241\n",
      "Iteration 750 loss 0.5945921922658776 train RMSE 1.502459853695353 Validation RMSE 750 : 1.4285135000730966 max of weights 0.8634046472237787\n",
      "Iteration 760 loss 0.6089346325320445 train RMSE 1.5387296106556891 Validation RMSE 760 : 1.3271135043460667 max of weights 0.8844944773000601\n",
      "Iteration 770 loss 0.5320787184231087 train RMSE 1.3440366554415304 Validation RMSE 770 : 1.2733240601688296 max of weights 0.8856475811145861\n",
      "Iteration 780 loss 0.5652701156260972 train RMSE 1.4280782460459112 Validation RMSE 780 : 1.3129144123217915 max of weights 0.8889961702957654\n",
      "Iteration 790 loss 0.5397445259220245 train RMSE 1.363392223041263 Validation RMSE 790 : 1.2269644536423754 max of weights 0.8906387157525729\n",
      "Iteration 800 loss 0.5589685778019973 train RMSE 1.412010322013967 Validation RMSE 800 : 1.245882916133737 max of weights 0.8890921568440918\n",
      "Iteration 810 loss 0.5607379887032158 train RMSE 1.4164859900767457 Validation RMSE 810 : 1.3041274137249241 max of weights 0.8971549216404179\n",
      "Iteration 820 loss 0.5539151175383596 train RMSE 1.399160469061327 Validation RMSE 820 : 1.2355368677903658 max of weights 0.8954481923146456\n",
      "Iteration 830 loss 0.5639316109263932 train RMSE 1.4244859291574206 Validation RMSE 830 : 1.21897177051493 max of weights 0.8937992256480122\n",
      "Iteration 840 loss 0.5484386672121679 train RMSE 1.3852132003466464 Validation RMSE 840 : 1.3053545896853778 max of weights 0.8948977758714178\n",
      "Iteration 850 loss 0.5539677383937164 train RMSE 1.3991599943434756 Validation RMSE 850 : 1.2553788296022084 max of weights 0.8877911318854542\n",
      "Iteration 860 loss 0.5675585013682162 train RMSE 1.433571812231256 Validation RMSE 860 : 1.2857274733944117 max of weights 0.8878226971469751\n",
      "Iteration 870 loss 0.5555635874734138 train RMSE 1.4031846536076251 Validation RMSE 870 : 1.3001962492993513 max of weights 0.8962294989570244\n",
      "Iteration 880 loss 0.5391574745220373 train RMSE 1.3615952677840475 Validation RMSE 880 : 1.2101867720645092 max of weights 0.9016946971265516\n",
      "Iteration 890 loss 0.5287848673468742 train RMSE 1.335289878033578 Validation RMSE 890 : 1.1505168132593375\n",
      "Neural test RMSE: 1.5694551676434279\n"
     ]
    }
   ],
   "source": [
    "# Example regression script using neural fingerprints.\n",
    "#\n",
    "# Compares Morgan fingerprints to neural fingerprints.\n",
    "\n",
    "import autograd.numpy as np\n",
    "import autograd.numpy.random as npr\n",
    "\n",
    "from neuralfingerprint import load_data\n",
    "from neuralfingerprint import build_morgan_deep_net\n",
    "from neuralfingerprint import build_conv_deep_net\n",
    "from neuralfingerprint import normalize_array, adam\n",
    "from neuralfingerprint import build_batched_grad\n",
    "from neuralfingerprint.util import rmse\n",
    "\n",
    "from autograd import grad\n",
    "\n",
    "task_params = {'target_name' : 'PCE',\n",
    "               'data_file'   : 'cep-processed.csv'}\n",
    "N_train = 19800\n",
    "N_val   = 100\n",
    "N_test  = 100\n",
    "\n",
    "model_params = dict(fp_length=50,    # Usually neural fps need far fewer dimensions than morgan.\n",
    "                    fp_depth=6,      # The depth of the network equals the fingerprint radius.\n",
    "                    conv_width=40,   # Only the neural fps need this parameter.\n",
    "                    h1_size=100,     # Size of hidden layer of network on top of fps.\n",
    "                    L2_reg=np.exp(-2))\n",
    "train_params = dict(num_iters=900,\n",
    "                    batch_size=100,\n",
    "                    init_scale=np.exp(-4),\n",
    "                    step_size=np.exp(-6))\n",
    "\n",
    "# Define the architecture of the network that sits on top of the fingerprints.\n",
    "vanilla_net_params = dict(\n",
    "    layer_sizes = [model_params['fp_length'], model_params['h1_size']],  # One hidden layer.\n",
    "    normalize=True, L2_reg = model_params['L2_reg'], nll_func = rmse)\n",
    "\n",
    "def train_nn(pred_fun, loss_fun, num_weights, train_smiles, train_raw_targets, train_params, seed=0,\n",
    "             validation_smiles=None, validation_raw_targets=None):\n",
    "    \"\"\"loss_fun has inputs (weights, smiles, targets)\"\"\"\n",
    "    print \"Total number of weights in the network:\", num_weights\n",
    "    init_weights = npr.RandomState(seed).randn(num_weights) * train_params['init_scale']\n",
    "\n",
    "    num_print_examples = 100\n",
    "    train_targets, undo_norm = normalize_array(train_raw_targets)\n",
    "    training_curve = []\n",
    "    def callback(weights, iter):\n",
    "        if iter % 10 == 0:\n",
    "            print \"max of weights\", np.max(np.abs(weights))\n",
    "            train_preds = undo_norm(pred_fun(weights, train_smiles[:num_print_examples]))\n",
    "            cur_loss = loss_fun(weights, train_smiles[:num_print_examples], train_targets[:num_print_examples])\n",
    "            training_curve.append(cur_loss)\n",
    "            print \"Iteration\", iter, \"loss\", cur_loss,\\\n",
    "                  \"train RMSE\", rmse(train_preds, train_raw_targets[:num_print_examples]),\n",
    "            if validation_smiles is not None:\n",
    "                validation_preds = undo_norm(pred_fun(weights, validation_smiles))\n",
    "                print \"Validation RMSE\", iter, \":\", rmse(validation_preds, validation_raw_targets),\n",
    "\n",
    "    # Build gradient using autograd.\n",
    "    grad_fun = grad(loss_fun)\n",
    "    grad_fun_with_data = build_batched_grad(grad_fun, train_params['batch_size'],\n",
    "                                            train_smiles, train_targets)\n",
    "\n",
    "    # Optimize weights.\n",
    "    trained_weights = adam(grad_fun_with_data, init_weights, callback=callback,\n",
    "                           num_iters=train_params['num_iters'], step_size=train_params['step_size'])\n",
    "\n",
    "    def predict_func(new_smiles):\n",
    "        \"\"\"Returns to the original units that the raw targets were in.\"\"\"\n",
    "        return undo_norm(pred_fun(trained_weights, new_smiles))\n",
    "    return predict_func, trained_weights, training_curve\n",
    "\n",
    "\n",
    "def main():\n",
    "    print \"Loading data...\"\n",
    "    traindata, valdata, testdata = load_data(\n",
    "        task_params['data_file'], (N_train, N_val, N_test),\n",
    "        input_name='smiles', target_name=task_params['target_name'])\n",
    "    train_inputs, train_targets = traindata\n",
    "    val_inputs,   val_targets   = valdata\n",
    "    test_inputs,  test_targets  = testdata\n",
    "\n",
    "    def print_performance(pred_func):\n",
    "        train_preds = pred_func(train_inputs)\n",
    "        val_preds = pred_func(val_inputs)\n",
    "        print \"\\nPerformance (RMSE) on \" + task_params['target_name'] + \":\"\n",
    "        print \"Train:\", rmse(train_preds, train_targets)\n",
    "        print \"Test: \", rmse(val_preds,  val_targets)\n",
    "        print \"-\" * 80\n",
    "        return rmse(val_preds, val_targets)\n",
    "\n",
    "    def run_morgan_experiment():\n",
    "        loss_fun, pred_fun, net_parser = \\\n",
    "            build_morgan_deep_net(model_params['fp_length'],\n",
    "                                  model_params['fp_depth'], vanilla_net_params)\n",
    "        num_weights = len(net_parser)\n",
    "        predict_func, trained_weights, conv_training_curve = \\\n",
    "            train_nn(pred_fun, loss_fun, num_weights, train_inputs, train_targets,\n",
    "                     train_params, validation_smiles=val_inputs, validation_raw_targets=val_targets)\n",
    "        return print_performance(predict_func)\n",
    "\n",
    "    def run_conv_experiment():\n",
    "        conv_layer_sizes = [model_params['conv_width']] * model_params['fp_depth']\n",
    "        conv_arch_params = {'num_hidden_features' : conv_layer_sizes,\n",
    "                            'fp_length' : model_params['fp_length'], 'normalize' : 1}\n",
    "        loss_fun, pred_fun, conv_parser = \\\n",
    "            build_conv_deep_net(conv_arch_params, vanilla_net_params, model_params['L2_reg'])\n",
    "        num_weights = len(conv_parser)\n",
    "        predict_func, trained_weights, conv_training_curve = \\\n",
    "            train_nn(pred_fun, loss_fun, num_weights, train_inputs, train_targets,\n",
    "                     train_params, validation_smiles=val_inputs, validation_raw_targets=val_targets)\n",
    "        test_predictions = predict_func(test_inputs)\n",
    "        return rmse(test_predictions, test_targets)\n",
    "\n",
    "    print \"Task params\", task_params\n",
    "    print\n",
    "    #print \"Starting Morgan fingerprint experiment...\"\n",
    "    #test_loss_morgan = run_morgan_experiment()\n",
    "    print \"Starting neural fingerprint experiment...\"\n",
    "    test_loss_neural = run_conv_experiment()\n",
    "    print\n",
    "    #print \"Morgan test RMSE:\", test_loss_morgan, \"Neural test RMSE:\", test_loss_neural\n",
    "    print \"Neural test RMSE:\", test_loss_neural\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Fingerprint drug efficacy result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Task params {'target_name': 'activity', 'data_file': 'malaria-processed.csv'}\n",
      "\n",
      "Starting neural fingerprint experiment...\n",
      "Total number of weights in the network: 41771\n",
      "max of weights 0.08535001578936458\n",
      "Iteration 0 loss 0.8187127378654213 train RMSE 0.9956669173433814 Validation RMSE 0 : 1.20552164057607 max of weights 0.07438252802890713\n",
      "Iteration 10 loss 0.8160962391088001 train RMSE 0.9925201814642607 Validation RMSE 10 : 1.2065253097589592 max of weights 0.0729394493817034\n",
      "Iteration 20 loss 0.815652419516367 train RMSE 0.9919839752081421 Validation RMSE 20 : 1.2084289317500778 max of weights 0.07753474261385916\n",
      "Iteration 30 loss 0.8151471304326946 train RMSE 0.9913712625474116 Validation RMSE 30 : 1.2083112679643948 max of weights 0.08506251631780189\n",
      "Iteration 40 loss 0.8148917593464117 train RMSE 0.9910563908448425 Validation RMSE 40 : 1.2094255061680563 max of weights 0.10069228476059333\n",
      "Iteration 50 loss 0.8153686514776366 train RMSE 0.9916278420309987 Validation RMSE 50 : 1.208085757198256 max of weights 0.11415551470497781\n",
      "Iteration 60 loss 0.8154100372116793 train RMSE 0.9916700463512872 Validation RMSE 60 : 1.2074971522620237 max of weights 0.1374732996292297\n",
      "Iteration 70 loss 0.8148510059345858 train RMSE 0.9909636627155354 Validation RMSE 70 : 1.206318136698028 max of weights 0.17244630793648913\n",
      "Iteration 80 loss 0.8191444279388908 train RMSE 0.9961067624808756 Validation RMSE 80 : 1.2019327341670716 max of weights 0.1956812342139452\n",
      "Iteration 90 loss 0.8055266503536748 train RMSE 0.9794195558929245 Validation RMSE 90 : 1.1825248772665766 max of weights 0.21417258623234356\n",
      "Iteration 100 loss 0.7724749763534866 train RMSE 0.9390636865370258 Validation RMSE 100 : 1.1067912234629569 max of weights 0.23663834429047118\n",
      "Iteration 110 loss 0.7549944477317218 train RMSE 0.9178146982330488 Validation RMSE 110 : 1.0419093395264392 max of weights 0.2555747613740551\n",
      "Iteration 120 loss 0.7577914194607244 train RMSE 0.9212006792494741 Validation RMSE 120 : 1.074062571727418 max of weights 0.2804931885999734\n",
      "Iteration 130 loss 0.7669644848912796 train RMSE 0.932309602916715 Validation RMSE 130 : 1.0072740162721678 max of weights 0.3039834030363612\n",
      "Iteration 140 loss 0.7428238576860257 train RMSE 0.90291905917691 Validation RMSE 140 : 1.0752646313556957 max of weights 0.3277287370664483\n",
      "Iteration 150 loss 0.7539629304659138 train RMSE 0.9164161478770931 Validation RMSE 150 : 1.0846920156655413 max of weights 0.3399168656802873\n",
      "Iteration 160 loss 0.7426799662380621 train RMSE 0.9026393784534212 Validation RMSE 160 : 1.0228537107903315 max of weights 0.3506264541914399\n",
      "Iteration 170 loss 0.7475243321882146 train RMSE 0.9084928631769079 Validation RMSE 170 : 1.0496795508336854 max of weights 0.3695622688292824\n",
      "Iteration 180 loss 0.7517022530444704 train RMSE 0.9135017892900509 Validation RMSE 180 : 1.044765477034151 max of weights 0.38142356364297064\n",
      "Iteration 190 loss 0.7374179030361194 train RMSE 0.8960739842127072 Validation RMSE 190 : 1.016023209625846 max of weights 0.3787485236097521\n",
      "Iteration 200 loss 0.7345279523646018 train RMSE 0.8925388442041274 Validation RMSE 200 : 1.0016948037821138 max of weights 0.37269003650474725\n",
      "Iteration 210 loss 0.7357709888928661 train RMSE 0.8940489463345156 Validation RMSE 210 : 1.057692770603686 max of weights 0.3573635968733659\n",
      "Iteration 220 loss 0.7338509830626003 train RMSE 0.8917224756086681 Validation RMSE 220 : 1.0469337939169294 max of weights 0.3598052232985222\n",
      "Iteration 230 loss 0.749145247590814 train RMSE 0.9103088370143994 Validation RMSE 230 : 1.0382126679211765 max of weights 0.3626185430624827\n",
      "Iteration 240 loss 0.74591320771378 train RMSE 0.9063350969081383 Validation RMSE 240 : 1.0789404889621854 max of weights 0.3587821913418182\n",
      "Iteration 250 loss 0.7331536653085687 train RMSE 0.8908349995337964 Validation RMSE 250 : 1.0699458763486829 max of weights 0.366440113002097\n",
      "Iteration 260 loss 0.731273939928097 train RMSE 0.8885402644511138 Validation RMSE 260 : 1.0525414448630537 max of weights 0.38330161978524563\n",
      "Iteration 270 loss 0.773647580769109 train RMSE 0.9400517981438793 Validation RMSE 270 : 1.0564441684375903 max of weights 0.39817174398169375\n",
      "Iteration 280 loss 0.7394445490846921 train RMSE 0.8984521658146948 Validation RMSE 280 : 1.0425951989531608 max of weights 0.4087411990664052\n",
      "Iteration 290 loss 0.7318260840397138 train RMSE 0.8891797532275566 Validation RMSE 290 : 1.0365422280266159 max of weights 0.40004621570083915\n",
      "Iteration 300 loss 0.7299486315042952 train RMSE 0.8868839405523027 Validation RMSE 300 : 1.0113992136449963 max of weights 0.3891761299145503\n",
      "Iteration 310 loss 0.7333344721679638 train RMSE 0.8910015870868735 Validation RMSE 310 : 1.0411399767025542 max of weights 0.41113493072061574\n",
      "Iteration 320 loss 0.7447949597671223 train RMSE 0.9049411631096678 Validation RMSE 320 : 1.0292245563761195 max of weights 0.4309480429590186\n",
      "Iteration 330 loss 0.7715932176886365 train RMSE 0.9375129890101243 Validation RMSE 330 : 1.0606358876229067 max of weights 0.46036885711977255\n",
      "Iteration 340 loss 0.7480584940998658 train RMSE 0.9088690964671936 Validation RMSE 340 : 1.073659900476251 max of weights 0.4735520971116654\n",
      "Iteration 350 loss 0.7415304965503685 train RMSE 0.9009411488895465 Validation RMSE 350 : 1.042273308974437 max of weights 0.47408110666281794\n",
      "Iteration 360 loss 0.7428365133771975 train RMSE 0.9025262449981379 Validation RMSE 360 : 1.0693558214762344 max of weights 0.46973800470254906\n",
      "Iteration 370 loss 0.7838363003891164 train RMSE 0.9523573079985083 Validation RMSE 370 : 1.0356532214301215 max of weights 0.47109141759141976\n",
      "Iteration 380 loss 0.7312650182067602 train RMSE 0.8884156415085878 Validation RMSE 380 : 1.0343376423651705 max of weights 0.4814897167649139\n",
      "Iteration 390 loss 0.7345038591528276 train RMSE 0.8923405513503159 Validation RMSE 390 : 1.045072093569691 max of weights 0.4983925132906083\n",
      "Iteration 400 loss 0.7331252506693651 train RMSE 0.8906555690768376 Validation RMSE 400 : 1.0304139778633579 max of weights 0.513291486521831\n",
      "Iteration 410 loss 0.7393144059580466 train RMSE 0.8981776618782595 Validation RMSE 410 : 1.050949834449619 max of weights 0.5363915661382848\n",
      "Iteration 420 loss 0.7609167820120025 train RMSE 0.9244596200368916 Validation RMSE 420 : 1.02596299767079 max of weights 0.5567475569485802\n",
      "Iteration 430 loss 0.7711367337256277 train RMSE 0.9368602908039074 Validation RMSE 430 : 1.0458662260658542 max of weights 0.587327083479558\n",
      "Iteration 440 loss 0.7443321202215377 train RMSE 0.9042511287995206 Validation RMSE 440 : 1.046864271836401 max of weights 0.596243957478997\n",
      "Iteration 450 loss 0.7551173797875083 train RMSE 0.9173694998812542 Validation RMSE 450 : 1.0360712793751525 max of weights 0.590140898500594\n",
      "Iteration 460 loss 0.771828875003141 train RMSE 0.9376945796129917 Validation RMSE 460 : 1.0836991686041615 max of weights 0.583739639452886\n",
      "Iteration 470 loss 0.7697649172522419 train RMSE 0.9351648713653425 Validation RMSE 470 : 1.021298265590641 max of weights 0.5867972044241982\n",
      "Iteration 480 loss 0.7332743844134302 train RMSE 0.8907783907901281 Validation RMSE 480 : 1.0311488821506625 max of weights 0.5976177457998141\n",
      "Iteration 490 loss 0.7407840745878235 train RMSE 0.8998920276690578 Validation RMSE 490 : 1.0284952675968195 max of weights 0.6134584939085298\n",
      "Iteration 500 loss 0.7448818791688062 train RMSE 0.9048670975329945 Validation RMSE 500 : 1.0418440204251165 max of weights 0.6264744653423623\n",
      "Iteration 510 loss 0.7542466189246675 train RMSE 0.9162515881782336 Validation RMSE 510 : 1.0426154513506232 max of weights 0.6425618214949752\n",
      "Iteration 520 loss 0.7672290871908426 train RMSE 0.9320460294158017 Validation RMSE 520 : 1.0276414622929497 max of weights 0.6626458545546117\n",
      "Iteration 530 loss 0.7652045483732012 train RMSE 0.9295498936338683 Validation RMSE 530 : 1.0404030344529178 max of weights 0.684169237539251\n",
      "Iteration 540 loss 0.7590017125538434 train RMSE 0.9220111364613587 Validation RMSE 540 : 1.0323371327250837 max of weights 0.6833665926786777\n",
      "Iteration 550 loss 0.7842966073314797 train RMSE 0.9527692417724204 Validation RMSE 550 : 1.0275434458470143 max of weights 0.6763836869608676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 560 loss 0.7853168442709713 train RMSE 0.954005780245009 Validation RMSE 560 : 1.0456857361616934 max of weights 0.6712329881638819\n",
      "Iteration 570 loss 0.7527061889067028 train RMSE 0.9143331923258904 Validation RMSE 570 : 1.0052514662885814 max of weights 0.6734677378312026\n",
      "Iteration 580 loss 0.7413384132872121 train RMSE 0.9004846407538187 Validation RMSE 580 : 1.0206834259232513 max of weights 0.6850321682887021\n",
      "Iteration 590 loss 0.7448763120606594 train RMSE 0.9047628291945121 Validation RMSE 590 : 1.0135108834598214 max of weights 0.6959057845606573\n",
      "Iteration 600 loss 0.7513463767098557 train RMSE 0.9126229287377792 Validation RMSE 600 : 1.052179052885087 max of weights 0.7090725303824328\n",
      "Iteration 610 loss 0.7584489380795324 train RMSE 0.9212696756931523 Validation RMSE 610 : 1.0270818631642775 max of weights 0.7192683607047061\n",
      "Iteration 620 loss 0.7640650972374164 train RMSE 0.928089973673023 Validation RMSE 620 : 1.0090551394278238 max of weights 0.7367281013935237\n",
      "Iteration 630 loss 0.7613420988556617 train RMSE 0.9247537360814504 Validation RMSE 630 : 1.033008448503697 max of weights 0.7466144563644531\n",
      "Iteration 640 loss 0.7689288146541509 train RMSE 0.9339874101581221 Validation RMSE 640 : 1.0222655896317479 max of weights 0.7395590329525704\n",
      "Iteration 650 loss 0.7733946566748333 train RMSE 0.9394123711140704 Validation RMSE 650 : 1.0162771735848677 max of weights 0.7301795599636438\n",
      "Iteration 660 loss 0.8078627457629501 train RMSE 0.9813191131994516 Validation RMSE 660 : 1.001494921705559 max of weights 0.7238611924888955\n",
      "Iteration 670 loss 0.7482718347231111 train RMSE 0.9088410871683408 Validation RMSE 670 : 0.9864220606298008 max of weights 0.7255077660564744\n",
      "Iteration 680 loss 0.7502146589532404 train RMSE 0.9111772498088218 Validation RMSE 680 : 1.0124819817122046 max of weights 0.7363801733731306\n",
      "Iteration 690 loss 0.7435301685481855 train RMSE 0.9030226343050275 Validation RMSE 690 : 1.005674161855512 max of weights 0.7451192576118878\n",
      "Iteration 700 loss 0.7453644604353998 train RMSE 0.9052487425327421 Validation RMSE 700 : 1.0301588349784 max of weights 0.7564307364946719\n",
      "Iteration 710 loss 0.7600934313049099 train RMSE 0.9231779168718887 Validation RMSE 710 : 1.001247668414376 max of weights 0.7647501542654874\n",
      "Iteration 720 loss 0.7724000929662924 train RMSE 0.938125711566082 Validation RMSE 720 : 0.9954733308973925 max of weights 0.7835296294773871\n",
      "Iteration 730 loss 0.7632353160857417 train RMSE 0.9269688657009596 Validation RMSE 730 : 0.9987570217554592 max of weights 0.7867268449839601\n",
      "Iteration 740 loss 0.7734709286949871 train RMSE 0.9394177188040291 Validation RMSE 740 : 1.009979012395553 max of weights 0.7742518103936639\n",
      "Iteration 750 loss 0.7757929540462131 train RMSE 0.9422335284434655 Validation RMSE 750 : 1.0162001485113814 max of weights 0.7633117250361486\n",
      "Iteration 760 loss 0.7932358220909813 train RMSE 0.9634251555247225 Validation RMSE 760 : 0.9860523511444149 max of weights 0.7583887683629076\n",
      "Iteration 770 loss 0.7499919906837491 train RMSE 0.9108288885855096 Validation RMSE 770 : 0.9823310512557435 max of weights 0.7603860625892938\n",
      "Iteration 780 loss 0.7415320148944491 train RMSE 0.900519485488387 Validation RMSE 780 : 0.9951260561671911 max of weights 0.7689349653766312\n",
      "Iteration 790 loss 0.7373187007245106 train RMSE 0.8953742077714848 Validation RMSE 790 : 1.0073957735476633\n",
      "Neural test RMSE: 1.0534474622904428\n"
     ]
    }
   ],
   "source": [
    "# Example regression script using neural fingerprints.\n",
    "#\n",
    "# Compares Morgan fingerprints to neural fingerprints.\n",
    "\n",
    "import autograd.numpy as np\n",
    "import autograd.numpy.random as npr\n",
    "\n",
    "from neuralfingerprint import load_data\n",
    "from neuralfingerprint import build_morgan_deep_net\n",
    "from neuralfingerprint import build_conv_deep_net\n",
    "from neuralfingerprint import normalize_array, adam\n",
    "from neuralfingerprint import build_batched_grad\n",
    "from neuralfingerprint.util import rmse\n",
    "\n",
    "from autograd import grad\n",
    "\n",
    "task_params = {'target_name' : 'activity',\n",
    "               'data_file'   : 'malaria-processed.csv'}\n",
    "N_train = 9700\n",
    "N_val   = 100\n",
    "N_test  = 100\n",
    "\n",
    "model_params = dict(fp_length=50,    # Usually neural fps need far fewer dimensions than morgan.\n",
    "                    fp_depth=6,      # The depth of the network equals the fingerprint radius.\n",
    "                    conv_width=20,   # Only the neural fps need this parameter.\n",
    "                    h1_size=100,     # Size of hidden layer of network on top of fps.\n",
    "                    L2_reg=np.exp(-2))\n",
    "train_params = dict(num_iters=800,\n",
    "                    batch_size=100,\n",
    "                    init_scale=np.exp(-4),\n",
    "                    step_size=np.exp(-6))\n",
    "\n",
    "# Define the architecture of the network that sits on top of the fingerprints.\n",
    "vanilla_net_params = dict(\n",
    "    layer_sizes = [model_params['fp_length'], model_params['h1_size']],  # One hidden layer.\n",
    "    normalize=True, L2_reg = model_params['L2_reg'], nll_func = rmse)\n",
    "\n",
    "def train_nn(pred_fun, loss_fun, num_weights, train_smiles, train_raw_targets, train_params, seed=0,\n",
    "             validation_smiles=None, validation_raw_targets=None):\n",
    "    \"\"\"loss_fun has inputs (weights, smiles, targets)\"\"\"\n",
    "    print \"Total number of weights in the network:\", num_weights\n",
    "    init_weights = npr.RandomState(seed).randn(num_weights) * train_params['init_scale']\n",
    "\n",
    "    num_print_examples = 100\n",
    "    train_targets, undo_norm = normalize_array(train_raw_targets)\n",
    "    training_curve = []\n",
    "    def callback(weights, iter):\n",
    "        if iter % 10 == 0:\n",
    "            print \"max of weights\", np.max(np.abs(weights))\n",
    "            train_preds = undo_norm(pred_fun(weights, train_smiles[:num_print_examples]))\n",
    "            cur_loss = loss_fun(weights, train_smiles[:num_print_examples], train_targets[:num_print_examples])\n",
    "            training_curve.append(cur_loss)\n",
    "            print \"Iteration\", iter, \"loss\", cur_loss,\\\n",
    "                  \"train RMSE\", rmse(train_preds, train_raw_targets[:num_print_examples]),\n",
    "            if validation_smiles is not None:\n",
    "                validation_preds = undo_norm(pred_fun(weights, validation_smiles))\n",
    "                print \"Validation RMSE\", iter, \":\", rmse(validation_preds, validation_raw_targets),\n",
    "\n",
    "    # Build gradient using autograd.\n",
    "    grad_fun = grad(loss_fun)\n",
    "    grad_fun_with_data = build_batched_grad(grad_fun, train_params['batch_size'],\n",
    "                                            train_smiles, train_targets)\n",
    "\n",
    "    # Optimize weights.\n",
    "    trained_weights = adam(grad_fun_with_data, init_weights, callback=callback,\n",
    "                           num_iters=train_params['num_iters'], step_size=train_params['step_size'])\n",
    "\n",
    "    def predict_func(new_smiles):\n",
    "        \"\"\"Returns to the original units that the raw targets were in.\"\"\"\n",
    "        return undo_norm(pred_fun(trained_weights, new_smiles))\n",
    "    return predict_func, trained_weights, training_curve\n",
    "\n",
    "\n",
    "def main():\n",
    "    print \"Loading data...\"\n",
    "    traindata, valdata, testdata = load_data(\n",
    "        task_params['data_file'], (N_train, N_val, N_test),\n",
    "        input_name='smiles', target_name=task_params['target_name'])\n",
    "    train_inputs, train_targets = traindata\n",
    "    val_inputs,   val_targets   = valdata\n",
    "    test_inputs,  test_targets  = testdata\n",
    "\n",
    "    def print_performance(pred_func):\n",
    "        train_preds = pred_func(train_inputs)\n",
    "        val_preds = pred_func(val_inputs)\n",
    "        print \"\\nPerformance (RMSE) on \" + task_params['target_name'] + \":\"\n",
    "        print \"Train:\", rmse(train_preds, train_targets)\n",
    "        print \"Test: \", rmse(val_preds,  val_targets)\n",
    "        print \"-\" * 80\n",
    "        return rmse(val_preds, val_targets)\n",
    "\n",
    "    def run_morgan_experiment():\n",
    "        loss_fun, pred_fun, net_parser = \\\n",
    "            build_morgan_deep_net(model_params['fp_length'],\n",
    "                                  model_params['fp_depth'], vanilla_net_params)\n",
    "        num_weights = len(net_parser)\n",
    "        predict_func, trained_weights, conv_training_curve = \\\n",
    "            train_nn(pred_fun, loss_fun, num_weights, train_inputs, train_targets,\n",
    "                     train_params, validation_smiles=val_inputs, validation_raw_targets=val_targets)\n",
    "        return print_performance(predict_func)\n",
    "\n",
    "    def run_conv_experiment():\n",
    "        conv_layer_sizes = [model_params['conv_width']] * model_params['fp_depth']\n",
    "        conv_arch_params = {'num_hidden_features' : conv_layer_sizes,\n",
    "                            'fp_length' : model_params['fp_length'], 'normalize' : 1}\n",
    "        loss_fun, pred_fun, conv_parser = \\\n",
    "            build_conv_deep_net(conv_arch_params, vanilla_net_params, model_params['L2_reg'])\n",
    "        num_weights = len(conv_parser)\n",
    "        predict_func, trained_weights, conv_training_curve = \\\n",
    "            train_nn(pred_fun, loss_fun, num_weights, train_inputs, train_targets,\n",
    "                     train_params, validation_smiles=val_inputs, validation_raw_targets=val_targets)\n",
    "        test_predictions = predict_func(test_inputs)\n",
    "        return rmse(test_predictions, test_targets)\n",
    "\n",
    "    print \"Task params\", task_params\n",
    "    print\n",
    "    #print \"Starting Morgan fingerprint experiment...\"\n",
    "    #test_loss_morgan = run_morgan_experiment()\n",
    "    print \"Starting neural fingerprint experiment...\"\n",
    "    test_loss_neural = run_conv_experiment()\n",
    "    print\n",
    "    #print \"Morgan test RMSE:\", test_loss_morgan, \"Neural test RMSE:\", test_loss_neural\n",
    "    print \"Neural test RMSE:\", test_loss_neural\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
