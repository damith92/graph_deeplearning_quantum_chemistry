{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Fingerprint Solubility result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Task params {'target_name': 'measured log solubility in mols per litre', 'data_file': 'delaney.csv'}\n",
      "\n",
      "Starting Morgan fingerprint experiment...\n",
      "Total number of weights in the network: 5201\n",
      "max of weights 0.06962983567500523\n",
      "Iteration 0 loss 1.0681550956230024 train RMSE 2.2262093968771457 Validation RMSE 0 : 2.5194018852922193 max of weights 0.07909914663176974\n",
      "Iteration 10 loss 0.9749391873591123 train RMSE 2.0318360131424322 Validation RMSE 10 : 2.281904008238235 max of weights 0.0966915817527744\n",
      "Iteration 20 loss 0.8955948789425274 train RMSE 1.8662927335768003 Validation RMSE 20 : 2.0896241502453066 max of weights 0.1159570060218332\n",
      "Iteration 30 loss 0.8286966024806365 train RMSE 1.726778759008268 Validation RMSE 30 : 1.9473471235843123 max of weights 0.12893227439072727\n",
      "Iteration 40 loss 0.7670027963527982 train RMSE 1.5980900498437653 Validation RMSE 40 : 1.8062391535848397 max of weights 0.1398175695778033\n",
      "Iteration 50 loss 0.7415104899426724 train RMSE 1.5448348633205784 Validation RMSE 50 : 1.7418220101093722 max of weights 0.14884850242569284\n",
      "Iteration 60 loss 0.7407748486894149 train RMSE 1.5432296217929855 Validation RMSE 60 : 1.7540961661456382 max of weights 0.15447421196146546\n",
      "Iteration 70 loss 0.7332032104803617 train RMSE 1.5274031834386828 Validation RMSE 70 : 1.7672412706788208 max of weights 0.15780408050718805\n",
      "Iteration 80 loss 0.7224628045878561 train RMSE 1.5049678603094698 Validation RMSE 80 : 1.7638059829014086 max of weights 0.1707542747546707\n",
      "Iteration 90 loss 0.7055946177050972 train RMSE 1.469757249269588 Validation RMSE 90 : 1.7591732044628128 max of weights 0.19536437980515545\n",
      "Iteration 100 loss 0.6933200780740733 train RMSE 1.4441128610204235 Validation RMSE 100 : 1.7565634165959148 max of weights 0.21732661064428355\n",
      "Iteration 110 loss 0.681756976872175 train RMSE 1.4199507021349322 Validation RMSE 110 : 1.754543085848455 max of weights 0.23596673849283567\n",
      "Iteration 120 loss 0.6734935294012171 train RMSE 1.4026606211641877 Validation RMSE 120 : 1.7562738059133047 max of weights 0.2537297190044189\n",
      "Iteration 130 loss 0.6556191047143899 train RMSE 1.3653318139366837 Validation RMSE 130 : 1.7551166459320469 max of weights 0.2641422057587537\n",
      "Iteration 140 loss 0.6412513838726853 train RMSE 1.3353036682336812 Validation RMSE 140 : 1.7515533373616385 max of weights 0.2744562000114878\n",
      "Iteration 150 loss 0.6283307475927197 train RMSE 1.3082887731628803 Validation RMSE 150 : 1.747643169645462 max of weights 0.2845532110564545\n",
      "Iteration 160 loss 0.6189152847783093 train RMSE 1.2885739827006426 Validation RMSE 160 : 1.7494467447834623 max of weights 0.2991357929275871\n",
      "Iteration 170 loss 0.5969473292063205 train RMSE 1.2426902412376273 Validation RMSE 170 : 1.7501934574261206 max of weights 0.318904157642143\n",
      "Iteration 180 loss 0.580229405435872 train RMSE 1.2077413630020155 Validation RMSE 180 : 1.7514635830167138 max of weights 0.3393080636740764\n",
      "Iteration 190 loss 0.5670873949585512 train RMSE 1.1802435787979926 Validation RMSE 190 : 1.7544192283444942 max of weights 0.359486838861471\n",
      "Iteration 200 loss 0.5585946468891352 train RMSE 1.1624317406139748 Validation RMSE 200 : 1.7599988832601308 max of weights 0.37984336710861705\n",
      "Iteration 210 loss 0.535822031327796 train RMSE 1.1148531204279342 Validation RMSE 210 : 1.7629715093642702 max of weights 0.3993161502288873\n",
      "Iteration 220 loss 0.5185248715637907 train RMSE 1.0786813216585331 Validation RMSE 220 : 1.7627274466903913 max of weights 0.4188494637031261\n",
      "Iteration 230 loss 0.5075857402200515 train RMSE 1.0557604551361268 Validation RMSE 230 : 1.7664121871502267 max of weights 0.4377081938150942\n",
      "Iteration 240 loss 0.5004305724602274 train RMSE 1.040724678324354 Validation RMSE 240 : 1.7812982159989823 max of weights 0.45668086379511\n",
      "Iteration 250 loss 0.4741695936225544 train RMSE 0.9858668616509211 Validation RMSE 250 : 1.789305842986731 max of weights 0.47511855491297994\n",
      "Iteration 260 loss 0.4573513948706438 train RMSE 0.950687780314448 Validation RMSE 260 : 1.7887756438315734 max of weights 0.4936956285620994\n",
      "Iteration 270 loss 0.44349396197347657 train RMSE 0.9216806278071794 Validation RMSE 270 : 1.7937988340599926 max of weights 0.5122640212018452\n",
      "Iteration 280 loss 0.43732892014163416 train RMSE 0.9087057963304257 Validation RMSE 280 : 1.8063984548471517 max of weights 0.5299789869870338\n",
      "Iteration 290 loss 0.40891081090431614 train RMSE 0.8493507311555538 Validation RMSE 290 : 1.8099554084634522 max of weights 0.5470498862362202\n",
      "Iteration 300 loss 0.3914401361367066 train RMSE 0.8128127805710705 Validation RMSE 300 : 1.79273591659511 max of weights 0.5636264177528437\n",
      "Iteration 310 loss 0.3783951953897843 train RMSE 0.785501423953674 Validation RMSE 310 : 1.7903768495939953 max of weights 0.5795524352637687\n",
      "Iteration 320 loss 0.37143913417753854 train RMSE 0.7708833751956294 Validation RMSE 320 : 1.7912444239522163 max of weights 0.5954411692638986\n",
      "Iteration 330 loss 0.34284162349547387 train RMSE 0.7111623358339393 Validation RMSE 330 : 1.7854889396361804 max of weights 0.6108234330772897\n",
      "Iteration 340 loss 0.3268583642290659 train RMSE 0.6777358144073137 Validation RMSE 340 : 1.7678300571380885 max of weights 0.6245162597827193\n",
      "Iteration 350 loss 0.31250999930488954 train RMSE 0.6477192323047679 Validation RMSE 350 : 1.7535159259247883 max of weights 0.6365401483915372\n",
      "Iteration 360 loss 0.31905501871753494 train RMSE 0.6612518114619772 Validation RMSE 360 : 1.7665483537714606 max of weights 0.647827332017042\n",
      "Iteration 370 loss 0.28973372284307697 train RMSE 0.6000385816834222 Validation RMSE 370 : 1.750231160003316 max of weights 0.6588281876770608\n",
      "Iteration 380 loss 0.27691854440462604 train RMSE 0.5732416834296596 Validation RMSE 380 : 1.7066725874943438 max of weights 0.6671899172409408\n",
      "Iteration 390 loss 0.3368941015011 train RMSE 0.6981678238277592 Validation RMSE 390 : 1.76186307540832 max of weights 0.6711492505842229\n",
      "Iteration 400 loss 0.2769642351341978 train RMSE 0.5731920816031416 Validation RMSE 400 : 1.7083322255681985 max of weights 0.6748261162486471\n",
      "Iteration 410 loss 0.25778082310366707 train RMSE 0.5331532941397107 Validation RMSE 410 : 1.7349255873030427 max of weights 0.6790890339930169\n",
      "Iteration 420 loss 0.23890587823174078 train RMSE 0.49374801565568494 Validation RMSE 420 : 1.6979773069799566 max of weights 0.6844725753733152\n",
      "Iteration 430 loss 0.24013167591281379 train RMSE 0.4962331496725513 Validation RMSE 430 : 1.6765494866222779 max of weights 0.6912456664261102\n",
      "Iteration 440 loss 0.28505443207200515 train RMSE 0.5897996862891174 Validation RMSE 440 : 1.726017101466432 max of weights 0.6957036871400578\n",
      "Iteration 450 loss 0.2600244941308067 train RMSE 0.5375745786736106 Validation RMSE 450 : 1.7402875406167866 max of weights 0.6992677419631812\n",
      "Iteration 460 loss 0.21973928401132262 train RMSE 0.4535575295101361 Validation RMSE 460 : 1.705112824129768 max of weights 0.7019924034021481\n",
      "Iteration 470 loss 0.19705009077582541 train RMSE 0.4062142778031147 Validation RMSE 470 : 1.6773614456081865 max of weights 0.704816237028479\n",
      "Iteration 480 loss 0.17760966823103588 train RMSE 0.3656411206181589 Validation RMSE 480 : 1.6591455431403739 max of weights 0.7083925056247806\n",
      "Iteration 490 loss 0.16342443028458234 train RMSE 0.3360256938838716 Validation RMSE 490 : 1.6494093256058857 max of weights 0.7112219820943463\n",
      "Iteration 500 loss 0.19102470238317087 train RMSE 0.3935050360229905 Validation RMSE 500 : 1.6986380623567328 max of weights 0.7149386042831356\n",
      "Iteration 510 loss 0.19377445258266243 train RMSE 0.3991928777767828 Validation RMSE 510 : 1.7102882515034004 max of weights 0.7174748522188483\n",
      "Iteration 520 loss 0.2017915902535341 train RMSE 0.41585267663066106 Validation RMSE 520 : 1.72090996553591 max of weights 0.7205598101202421\n",
      "Iteration 530 loss 0.18336587269817634 train RMSE 0.37740286432938713 Validation RMSE 530 : 1.6979025135105037 max of weights 0.7229112006712417\n",
      "Iteration 540 loss 0.18107809992253798 train RMSE 0.37259323873445765 Validation RMSE 540 : 1.684208641526212 max of weights 0.7257578674045401\n",
      "Iteration 550 loss 0.2156724789431978 train RMSE 0.44465935324506284 Validation RMSE 550 : 1.7118167984417985 max of weights 0.7285722122384902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 560 loss 0.17035132784767495 train RMSE 0.35016633345118026 Validation RMSE 560 : 1.7010086923055947 max of weights 0.7315450763488113\n",
      "Iteration 570 loss 0.12901928984381208 train RMSE 0.2639813332488222 Validation RMSE 570 : 1.6694959391218331 max of weights 0.7356953376594595\n",
      "Iteration 580 loss 0.16225667451621806 train RMSE 0.33322024942168205 Validation RMSE 580 : 1.6442775620411383 max of weights 0.7381066295062453\n",
      "Iteration 590 loss 0.18482287273122106 train RMSE 0.3802212719850046 Validation RMSE 590 : 1.663453658380517 max of weights 0.7389376570466474\n",
      "Iteration 600 loss 0.15566803231868245 train RMSE 0.31942569859777936 Validation RMSE 600 : 1.6555307538734039 max of weights 0.7413988281981054\n",
      "Iteration 610 loss 0.14167916908206007 train RMSE 0.2902390980190949 Validation RMSE 610 : 1.6638955741749661 max of weights 0.7432117087449247\n",
      "Iteration 620 loss 0.12241926393092051 train RMSE 0.2500580915412507 Validation RMSE 620 : 1.6530758520527415 max of weights 0.7461883798886418\n",
      "Iteration 630 loss 0.114937252546925 train RMSE 0.23442539203357565 Validation RMSE 630 : 1.641279103305263 max of weights 0.7481250597322521\n",
      "Iteration 640 loss 0.11157817798906701 train RMSE 0.2273940296731557 Validation RMSE 640 : 1.6418331750624662 max of weights 0.749076255478184\n",
      "Iteration 650 loss 0.12916925206696858 train RMSE 0.2640345613143902 Validation RMSE 650 : 1.634864327452594 max of weights 0.7500144402336272\n",
      "Iteration 660 loss 0.09548263596790794 train RMSE 0.19380080901282445 Validation RMSE 660 : 1.6646485890820788 max of weights 0.7514165921152055\n",
      "Iteration 670 loss 0.09763166384960406 train RMSE 0.19824620713158464 Validation RMSE 670 : 1.672287684995445 max of weights 0.7544645525402666\n",
      "Iteration 680 loss 0.1226436113733935 train RMSE 0.250335913590111 Validation RMSE 680 : 1.6711892221640225 max of weights 0.7568931711765472\n",
      "Iteration 690 loss 0.10736037707325875 train RMSE 0.21845059869097244 Validation RMSE 690 : 1.686549408394698 max of weights 0.7575240073573835\n",
      "Iteration 700 loss 0.13476046980865417 train RMSE 0.2755333026314135 Validation RMSE 700 : 1.6800899714619093 max of weights 0.7581815435760001\n",
      "Iteration 710 loss 0.14185760308035952 train RMSE 0.2903103752330777 Validation RMSE 710 : 1.6824266820661649 max of weights 0.7604098494279099\n",
      "Iteration 720 loss 0.0816281747620364 train RMSE 0.1647611168619913 Validation RMSE 720 : 1.6534547378729134 max of weights 0.7617090110996617\n",
      "Iteration 730 loss 0.06833370252599623 train RMSE 0.1370158709280507 Validation RMSE 730 : 1.6376148506858077 max of weights 0.7662275372377662\n",
      "Iteration 740 loss 0.08256716418359353 train RMSE 0.1666474859276579 Validation RMSE 740 : 1.6354106209674315 max of weights 0.7684173319483666\n",
      "Iteration 750 loss 0.06109382675065307 train RMSE 0.12186314211917998 Validation RMSE 750 : 1.6358618364459911 max of weights 0.7698085026076718\n",
      "Iteration 760 loss 0.08615970193607873 train RMSE 0.17408699690400575 Validation RMSE 760 : 1.642344489393856 max of weights 0.77004519852274\n",
      "Iteration 770 loss 0.12308893987443546 train RMSE 0.2510450877366003 Validation RMSE 770 : 1.627537625702059 max of weights 0.7702398066387628\n",
      "Iteration 780 loss 0.07332695045119282 train RMSE 0.14731418158800705 Validation RMSE 780 : 1.6649038387755155 max of weights 0.7711249248033352\n",
      "Iteration 790 loss 0.06193523527052346 train RMSE 0.12354434341877303 Validation RMSE 790 : 1.663856185893801 \n",
      "Performance (RMSE) on measured log solubility in mols per litre:\n",
      "Train: 0.309505261413564\n",
      "Test:  1.6497376392343857\n",
      "--------------------------------------------------------------------------------\n",
      "Starting neural fingerprint experiment...\n",
      "Total number of weights in the network: 41771\n",
      "max of weights 0.08535001578936458\n",
      "Iteration 0 loss 1.0697176407933626 train RMSE 2.229371357210592 Validation RMSE 0 : 2.5193902064257268 max of weights 0.08801359268927947\n",
      "Iteration 10 loss 0.8727030711087946 train RMSE 1.818658890654749 Validation RMSE 10 : 1.9411727325876165 max of weights 0.10745615730208936\n",
      "Iteration 20 loss 0.8724936145314434 train RMSE 1.8180963131395589 Validation RMSE 20 : 1.9929411253253628 max of weights 0.1364283825680823\n",
      "Iteration 30 loss 0.8159650433822839 train RMSE 1.700038264973714 Validation RMSE 30 : 1.8721452349218513 max of weights 0.16864242471731006\n",
      "Iteration 40 loss 0.6619040771459368 train RMSE 1.3785909358035973 Validation RMSE 40 : 1.5555106812503108 max of weights 0.18934722411731258\n",
      "Iteration 50 loss 0.5912132948928159 train RMSE 1.231056643652191 Validation RMSE 50 : 1.5125882991221207 max of weights 0.21319383531722366\n",
      "Iteration 60 loss 0.541971391657829 train RMSE 1.1281705112195897 Validation RMSE 60 : 1.4222882075844439 max of weights 0.24076775078258236\n",
      "Iteration 70 loss 0.520856321449845 train RMSE 1.0839844703877635 Validation RMSE 70 : 1.4398765054604665 max of weights 0.2733818278168817\n",
      "Iteration 80 loss 0.4953283943056727 train RMSE 1.0305710339726077 Validation RMSE 80 : 1.401321682299106 max of weights 0.3047124283186235\n",
      "Iteration 90 loss 0.456578437514101 train RMSE 0.949701071010209 Validation RMSE 90 : 1.2465091596895401 max of weights 0.3346202144361005\n",
      "Iteration 100 loss 0.42596541074394134 train RMSE 0.8857852273947958 Validation RMSE 100 : 1.090247572027871 max of weights 0.35923842806133166\n",
      "Iteration 110 loss 0.40938541428079245 train RMSE 0.8511470314856954 Validation RMSE 110 : 0.9645646008410224 max of weights 0.37883881965289035\n",
      "Iteration 120 loss 0.4000520431040002 train RMSE 0.8316104611882598 Validation RMSE 120 : 0.8983733412092203 max of weights 0.3923783086735666\n",
      "Iteration 130 loss 0.38263178474638476 train RMSE 0.79523466848122 Validation RMSE 130 : 0.9105067578723622 max of weights 0.40405414557548636\n",
      "Iteration 140 loss 0.3606743756631936 train RMSE 0.7493779888600541 Validation RMSE 140 : 0.7928534126962947 max of weights 0.4116860932657662\n",
      "Iteration 150 loss 0.36243740615442627 train RMSE 0.7529935259360454 Validation RMSE 150 : 0.754849420075582 max of weights 0.41838411619529586\n",
      "Iteration 160 loss 0.3643861710193776 train RMSE 0.7569931404755601 Validation RMSE 160 : 0.7377721607456751 max of weights 0.4248952118918168\n",
      "Iteration 170 loss 0.33496569865891274 train RMSE 0.6956372490840775 Validation RMSE 170 : 0.7454119103045662 max of weights 0.428422264403195\n",
      "Iteration 180 loss 0.32701540182854516 train RMSE 0.6790384392716831 Validation RMSE 180 : 0.7203502843226121 max of weights 0.431172467626701\n",
      "Iteration 190 loss 0.3300542159042599 train RMSE 0.6853626274654572 Validation RMSE 190 : 0.7260519669040489 max of weights 0.43384371756788576\n",
      "Iteration 200 loss 0.33599904746140963 train RMSE 0.6977383843917448 Validation RMSE 200 : 0.7103688240760468 max of weights 0.4354021561147821\n",
      "Iteration 210 loss 0.3155289071055485 train RMSE 0.6550460481196836 Validation RMSE 210 : 0.7061472301622248 max of weights 0.4383264564713079\n",
      "Iteration 220 loss 0.3073557031885034 train RMSE 0.6379847509370584 Validation RMSE 220 : 0.6956218590082487 max of weights 0.4492985159377824\n",
      "Iteration 230 loss 0.31525376792777643 train RMSE 0.6544198034182044 Validation RMSE 230 : 0.6710171067605252 max of weights 0.47178725967279056\n",
      "Iteration 240 loss 0.31879083735618374 train RMSE 0.6617611319256916 Validation RMSE 240 : 0.671593295943452 max of weights 0.49147837587044535\n",
      "Iteration 250 loss 0.3139394744325889 train RMSE 0.6516106461300422 Validation RMSE 250 : 0.7650183589293533 max of weights 0.5093421625740924\n",
      "Iteration 260 loss 0.30259148411647524 train RMSE 0.6279090058272415 Validation RMSE 260 : 0.7107255255141135 max of weights 0.5216521284129854\n",
      "Iteration 270 loss 0.30477935156326197 train RMSE 0.6324262780574393 Validation RMSE 270 : 0.6483620534484137 max of weights 0.5351176012763192\n",
      "Iteration 280 loss 0.3201970142205495 train RMSE 0.6645137771395359 Validation RMSE 280 : 0.6461288122139615 max of weights 0.5515963632157304\n",
      "Iteration 290 loss 0.29140548308885417 train RMSE 0.6044642534428266 Validation RMSE 290 : 0.6391511897504499 max of weights 0.5681648153219395\n",
      "Iteration 300 loss 0.29532362203372353 train RMSE 0.6125890686443738 Validation RMSE 300 : 0.6951286533281568 max of weights 0.598110662171087\n",
      "Iteration 310 loss 0.314936511710105 train RMSE 0.6534371139565852 Validation RMSE 310 : 0.7859476532673491 max of weights 0.6279677020055575\n",
      "Iteration 320 loss 0.30539815109530205 train RMSE 0.6335092056771326 Validation RMSE 320 : 0.6923587327083444 max of weights 0.6605253370308629\n",
      "Iteration 330 loss 0.2881288215860607 train RMSE 0.5974600141908227 Validation RMSE 330 : 0.6252321099828724 max of weights 0.6946977788865317\n",
      "Iteration 340 loss 0.30346666590384813 train RMSE 0.6293731769657932 Validation RMSE 340 : 0.6207661883282342 max of weights 0.7294676512473766\n",
      "Iteration 350 loss 0.3129138005405236 train RMSE 0.6490197317585493 Validation RMSE 350 : 0.6126390652929566 max of weights 0.7641610111914408\n",
      "Iteration 360 loss 0.3134648709361477 train RMSE 0.6501138464107701 Validation RMSE 360 : 0.6128899427587283 max of weights 0.7919861271469041\n",
      "Iteration 370 loss 0.29056305602219135 train RMSE 0.6023207922133144 Validation RMSE 370 : 0.6309943674453364 max of weights 0.8170613307046313\n",
      "Iteration 380 loss 0.2902068556537281 train RMSE 0.6015162900988189 Validation RMSE 380 : 0.6540581051999513 max of weights 0.842973122261202\n",
      "Iteration 390 loss 0.294311624675125 train RMSE 0.6100288819577995 Validation RMSE 390 : 0.6911050539666819 max of weights 0.8720983587866619\n",
      "Iteration 400 loss 0.30221788127382654 train RMSE 0.6264678874639075 Validation RMSE 400 : 0.679795837947056 max of weights 0.902266286393677\n",
      "Iteration 410 loss 0.28019676045517594 train RMSE 0.5805273476852258 Validation RMSE 410 : 0.6492123743939591 max of weights 0.9324039230930049\n",
      "Iteration 420 loss 0.2827881719244491 train RMSE 0.5858853164866357 Validation RMSE 420 : 0.6072823224884432 max of weights 0.9581398429296477\n",
      "Iteration 430 loss 0.31192811081722244 train RMSE 0.6465791447855912 Validation RMSE 430 : 0.5721812128735125 max of weights 0.9790230221149627\n",
      "Iteration 440 loss 0.3308268395417265 train RMSE 0.6859316078490525 Validation RMSE 440 : 0.6113120378489477 max of weights 0.9992115398326444\n",
      "Iteration 450 loss 0.2980386896825517 train RMSE 0.6175400721662836 Validation RMSE 450 : 0.5868385304917695 max of weights 1.022814528137061\n",
      "Iteration 460 loss 0.28327778981330126 train RMSE 0.586737110436462 Validation RMSE 460 : 0.5852790589193716 max of weights 1.045332551493903\n",
      "Iteration 470 loss 0.2914649799804942 train RMSE 0.603771731527127 Validation RMSE 470 : 0.6300831271645324 max of weights 1.067138743268856\n",
      "Iteration 480 loss 0.29935257884201705 train RMSE 0.6201757017708304 Validation RMSE 480 : 0.6433761741476954 max of weights 1.0893463826183631\n",
      "Iteration 490 loss 0.2870655576425878 train RMSE 0.5945296553429533 Validation RMSE 490 : 0.6785429833604953 max of weights 1.1097845096271557\n",
      "Iteration 500 loss 0.29335258297454453 train RMSE 0.6075927645493236 Validation RMSE 500 : 0.6654194791565856 max of weights 1.1258700847329735\n",
      "Iteration 510 loss 0.2945585228829042 train RMSE 0.6100734068919178 Validation RMSE 510 : 0.5914199464383535 max of weights 1.1417941418799238\n",
      "Iteration 520 loss 0.3060848351917198 train RMSE 0.6340610976399479 Validation RMSE 520 : 0.5824614684740901 max of weights 1.1587510198390114\n",
      "Iteration 530 loss 0.28868758469573075 train RMSE 0.597768842707335 Validation RMSE 530 : 0.572511557366105 max of weights 1.1742543276766853\n",
      "Iteration 540 loss 0.28149164507185026 train RMSE 0.582733160809829 Validation RMSE 540 : 0.5650265617115097 max of weights 1.1902586379274154\n",
      "Iteration 550 loss 0.28993045270206025 train RMSE 0.6003087065779389 Validation RMSE 550 : 0.6525635510239955 max of weights 1.2093627399758127\n",
      "Iteration 560 loss 0.2989913685494695 train RMSE 0.6191623027362443 Validation RMSE 560 : 0.6591892147922204 max of weights 1.2296413977814558\n",
      "Iteration 570 loss 0.2792440385864196 train RMSE 0.5779712265110482 Validation RMSE 570 : 0.68292215574723 max of weights 1.2475292773402031\n",
      "Iteration 580 loss 0.277480864819003 train RMSE 0.5742476659386893 Validation RMSE 580 : 0.6210593996374153 max of weights 1.2650966277910207\n",
      "Iteration 590 loss 0.29695921024849775 train RMSE 0.6148119011407169 Validation RMSE 590 : 0.5919739537169445 max of weights 1.282632236510588\n",
      "Iteration 600 loss 0.310243869030995 train RMSE 0.6424664869144091 Validation RMSE 600 : 0.6042687119514848 max of weights 1.2969003278224045\n",
      "Iteration 610 loss 0.2772888559056407 train RMSE 0.5737406179101282 Validation RMSE 610 : 0.579214405900348 max of weights 1.3101037745045963\n",
      "Iteration 620 loss 0.28078739473236025 train RMSE 0.5810063546849642 Validation RMSE 620 : 0.6413553128682755 max of weights 1.3248127564680214\n",
      "Iteration 630 loss 0.3052223401772711 train RMSE 0.6319022666177536 Validation RMSE 630 : 0.7171235127558496 max of weights 1.3408545218913444\n",
      "Iteration 640 loss 0.2989751677833047 train RMSE 0.618843082752936 Validation RMSE 640 : 0.7056954961023036 max of weights 1.3545464400883258\n",
      "Iteration 650 loss 0.27469409989915067 train RMSE 0.5681950110189397 Validation RMSE 650 : 0.6594855403577039 max of weights 1.3722870057662266\n",
      "Iteration 660 loss 0.27984532815794383 train RMSE 0.5788853927334696 Validation RMSE 660 : 0.6535679113026643 max of weights 1.3865774122882644\n",
      "Iteration 670 loss 0.28083244807705077 train RMSE 0.5809096787415671 Validation RMSE 670 : 0.5931856012594019 max of weights 1.3992165943721209\n",
      "Iteration 680 loss 0.29647192018504 train RMSE 0.6134768177452447 Validation RMSE 680 : 0.5692089497732762 max of weights 1.4137165427966598\n",
      "Iteration 690 loss 0.283230174227067 train RMSE 0.5858367541509787 Validation RMSE 690 : 0.567726597382753 max of weights 1.4292988180337773\n",
      "Iteration 700 loss 0.27503755383844564 train RMSE 0.5687371297778637 Validation RMSE 700 : 0.5824010862254656 max of weights 1.442091953840523\n",
      "Iteration 710 loss 0.2754826080444469 train RMSE 0.5696475254742567 Validation RMSE 710 : 0.6110914414372641 max of weights 1.4555213863354939\n",
      "Iteration 720 loss 0.2874126779462906 train RMSE 0.5944846873796091 Validation RMSE 720 : 0.6433878132440422 max of weights 1.4697851120401135\n",
      "Iteration 730 loss 0.27336987502406385 train RMSE 0.5651959348317533 Validation RMSE 730 : 0.70306076712699 max of weights 1.48169078784033\n",
      "Iteration 740 loss 0.2725193697516403 train RMSE 0.5633901240507092 Validation RMSE 740 : 0.6434700177739308 max of weights 1.4957340924322726\n",
      "Iteration 750 loss 0.28312973703199473 train RMSE 0.5854676271324422 Validation RMSE 750 : 0.5806230338921776 max of weights 1.508931475063298\n",
      "Iteration 760 loss 0.2930243567031664 train RMSE 0.6060574411638594 Validation RMSE 760 : 0.5755627137655444 max of weights 1.517807984751242\n",
      "Iteration 770 loss 0.28079422138884436 train RMSE 0.5805343209445594 Validation RMSE 770 : 0.5554184700257682 max of weights 1.526001501674409\n",
      "Iteration 780 loss 0.27810765665189824 train RMSE 0.5749081043888522 Validation RMSE 780 : 0.6197152249749569 max of weights 1.5354077703535096\n",
      "Iteration 790 loss 0.2771235237370147 train RMSE 0.5728421331110672 Validation RMSE 790 : 0.6469348891655811\n",
      "Morgan test RMSE: 1.6497376392343857 Neural test RMSE: 0.57439090707962\n"
     ]
    }
   ],
   "source": [
    "# Example regression script using neural fingerprints.\n",
    "#\n",
    "# Compares Morgan fingerprints to neural fingerprints.\n",
    "\n",
    "import autograd.numpy as np\n",
    "import autograd.numpy.random as npr\n",
    "\n",
    "from neuralfingerprint import load_data\n",
    "from neuralfingerprint import build_morgan_deep_net\n",
    "from neuralfingerprint import build_conv_deep_net\n",
    "from neuralfingerprint import normalize_array, adam\n",
    "from neuralfingerprint import build_batched_grad\n",
    "from neuralfingerprint.util import rmse\n",
    "\n",
    "from autograd import grad\n",
    "\n",
    "task_params = {'target_name' : 'measured log solubility in mols per litre',\n",
    "               'data_file'   : 'delaney.csv'}\n",
    "N_train = 800\n",
    "N_val   = 20\n",
    "N_test  = 20\n",
    "\n",
    "model_params = dict(fp_length=50,    # Usually neural fps need far fewer dimensions than morgan.\n",
    "                    fp_depth=6,      # The depth of the network equals the fingerprint radius.\n",
    "                    conv_width=20,   # Only the neural fps need this parameter.\n",
    "                    h1_size=100,     # Size of hidden layer of network on top of fps.\n",
    "                    L2_reg=np.exp(-2))\n",
    "train_params = dict(num_iters=800,\n",
    "                    batch_size=100,\n",
    "                    init_scale=np.exp(-4),\n",
    "                    step_size=np.exp(-6))\n",
    "\n",
    "# Define the architecture of the network that sits on top of the fingerprints.\n",
    "vanilla_net_params = dict(\n",
    "    layer_sizes = [model_params['fp_length'], model_params['h1_size']],  # One hidden layer.\n",
    "    normalize=True, L2_reg = model_params['L2_reg'], nll_func = rmse)\n",
    "\n",
    "def train_nn(pred_fun, loss_fun, num_weights, train_smiles, train_raw_targets, train_params, seed=0,\n",
    "             validation_smiles=None, validation_raw_targets=None):\n",
    "    \"\"\"loss_fun has inputs (weights, smiles, targets)\"\"\"\n",
    "    print \"Total number of weights in the network:\", num_weights\n",
    "    init_weights = npr.RandomState(seed).randn(num_weights) * train_params['init_scale']\n",
    "\n",
    "    num_print_examples = 100\n",
    "    train_targets, undo_norm = normalize_array(train_raw_targets)\n",
    "    training_curve = []\n",
    "    def callback(weights, iter):\n",
    "        if iter % 10 == 0:\n",
    "            print \"max of weights\", np.max(np.abs(weights))\n",
    "            train_preds = undo_norm(pred_fun(weights, train_smiles[:num_print_examples]))\n",
    "            cur_loss = loss_fun(weights, train_smiles[:num_print_examples], train_targets[:num_print_examples])\n",
    "            training_curve.append(cur_loss)\n",
    "            print \"Iteration\", iter, \"loss\", cur_loss,\\\n",
    "                  \"train RMSE\", rmse(train_preds, train_raw_targets[:num_print_examples]),\n",
    "            if validation_smiles is not None:\n",
    "                validation_preds = undo_norm(pred_fun(weights, validation_smiles))\n",
    "                print \"Validation RMSE\", iter, \":\", rmse(validation_preds, validation_raw_targets),\n",
    "\n",
    "    # Build gradient using autograd.\n",
    "    grad_fun = grad(loss_fun)\n",
    "    grad_fun_with_data = build_batched_grad(grad_fun, train_params['batch_size'],\n",
    "                                            train_smiles, train_targets)\n",
    "\n",
    "    # Optimize weights.\n",
    "    trained_weights = adam(grad_fun_with_data, init_weights, callback=callback,\n",
    "                           num_iters=train_params['num_iters'], step_size=train_params['step_size'])\n",
    "\n",
    "    def predict_func(new_smiles):\n",
    "        \"\"\"Returns to the original units that the raw targets were in.\"\"\"\n",
    "        return undo_norm(pred_fun(trained_weights, new_smiles))\n",
    "    return predict_func, trained_weights, training_curve\n",
    "\n",
    "\n",
    "def main():\n",
    "    print \"Loading data...\"\n",
    "    traindata, valdata, testdata = load_data(\n",
    "        task_params['data_file'], (N_train, N_val, N_test),\n",
    "        input_name='smiles', target_name=task_params['target_name'])\n",
    "    train_inputs, train_targets = traindata\n",
    "    val_inputs,   val_targets   = valdata\n",
    "    test_inputs,  test_targets  = testdata\n",
    "\n",
    "    def print_performance(pred_func):\n",
    "        train_preds = pred_func(train_inputs)\n",
    "        val_preds = pred_func(val_inputs)\n",
    "        print \"\\nPerformance (RMSE) on \" + task_params['target_name'] + \":\"\n",
    "        print \"Train:\", rmse(train_preds, train_targets)\n",
    "        print \"Test: \", rmse(val_preds,  val_targets)\n",
    "        print \"-\" * 80\n",
    "        return rmse(val_preds, val_targets)\n",
    "\n",
    "    def run_morgan_experiment():\n",
    "        loss_fun, pred_fun, net_parser = \\\n",
    "            build_morgan_deep_net(model_params['fp_length'],\n",
    "                                  model_params['fp_depth'], vanilla_net_params)\n",
    "        num_weights = len(net_parser)\n",
    "        predict_func, trained_weights, conv_training_curve = \\\n",
    "            train_nn(pred_fun, loss_fun, num_weights, train_inputs, train_targets,\n",
    "                     train_params, validation_smiles=val_inputs, validation_raw_targets=val_targets)\n",
    "        return print_performance(predict_func)\n",
    "\n",
    "    def run_conv_experiment():\n",
    "        conv_layer_sizes = [model_params['conv_width']] * model_params['fp_depth']\n",
    "        conv_arch_params = {'num_hidden_features' : conv_layer_sizes,\n",
    "                            'fp_length' : model_params['fp_length'], 'normalize' : 1}\n",
    "        loss_fun, pred_fun, conv_parser = \\\n",
    "            build_conv_deep_net(conv_arch_params, vanilla_net_params, model_params['L2_reg'])\n",
    "        num_weights = len(conv_parser)\n",
    "        predict_func, trained_weights, conv_training_curve = \\\n",
    "            train_nn(pred_fun, loss_fun, num_weights, train_inputs, train_targets,\n",
    "                     train_params, validation_smiles=val_inputs, validation_raw_targets=val_targets)\n",
    "        test_predictions = predict_func(test_inputs)\n",
    "        return rmse(test_predictions, test_targets)\n",
    "\n",
    "    print \"Task params\", task_params\n",
    "    print\n",
    "    print \"Starting Morgan fingerprint experiment...\"\n",
    "    test_loss_morgan = run_morgan_experiment()\n",
    "    print \"Starting neural fingerprint experiment...\"\n",
    "    test_loss_neural = run_conv_experiment()\n",
    "    print\n",
    "    print \"Morgan test RMSE:\", test_loss_morgan, \"Neural test RMSE:\", test_loss_neural\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Fingerprint photovoltaic efficiency result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Task params {'target_name': 'PCE', 'data_file': 'cep-processed.csv'}\n",
      "\n",
      "Starting Morgan fingerprint experiment...\n",
      "Total number of weights in the network: 5201\n",
      "max of weights 0.06962983567500523\n",
      "Iteration 0 loss 0.9385766593076806 train RMSE 2.376559308764014 Validation RMSE 0 : 2.3435034177697256 max of weights 0.07283979395826158\n",
      "Iteration 10 loss 0.9160849416948444 train RMSE 2.3195707149312503 Validation RMSE 10 : 2.3076133365618023 max of weights 0.09576515212845581\n",
      "Iteration 20 loss 0.8888843114093663 train RMSE 2.250610964861513 Validation RMSE 20 : 2.248498139613563 max of weights 0.11674110323273869\n",
      "Iteration 30 loss 0.8486465259682635 train RMSE 2.1485791633614855 Validation RMSE 30 : 2.165018351769259 max of weights 0.13085101807156044\n",
      "Iteration 40 loss 0.8167243858342416 train RMSE 2.0675849578779233 Validation RMSE 40 : 2.113094017779867 max of weights 0.1439236181565618\n",
      "Iteration 50 loss 0.8025603580531296 train RMSE 2.031602155485496 Validation RMSE 50 : 2.0977915069073956 max of weights 0.15258360225272669\n",
      "Iteration 60 loss 0.7993289065315761 train RMSE 2.023380639588945 Validation RMSE 60 : 2.0931062267156104 max of weights 0.15978741624058973\n",
      "Iteration 70 loss 0.8094754672672991 train RMSE 2.0490458547409443 Validation RMSE 70 : 2.1068304074059343 max of weights 0.1669745139157295\n",
      "Iteration 80 loss 0.8135765308558238 train RMSE 2.059402930587756 Validation RMSE 80 : 2.107514175709854 max of weights 0.16728425376235065\n",
      "Iteration 90 loss 0.8130914183460509 train RMSE 2.0581493904948256 Validation RMSE 90 : 2.1095825461207336 max of weights 0.1728889874151429\n",
      "Iteration 100 loss 0.8116378211441809 train RMSE 2.0544203429112633 Validation RMSE 100 : 2.1073587891780057 max of weights 0.18044664298245322\n",
      "Iteration 110 loss 0.816550169832855 train RMSE 2.066856756564427 Validation RMSE 110 : 2.094107752847888 max of weights 0.19240820715577525\n",
      "Iteration 120 loss 0.8240337319573793 train RMSE 2.0857878892518618 Validation RMSE 120 : 2.0912916766013505 max of weights 0.20361452285617088\n",
      "Iteration 130 loss 0.8213475901305135 train RMSE 2.0789316671447757 Validation RMSE 130 : 2.0747230463016693 max of weights 0.2120803290083264\n",
      "Iteration 140 loss 0.8202274034107577 train RMSE 2.076055199824005 Validation RMSE 140 : 2.0784982278673345 max of weights 0.21469097112723917\n",
      "Iteration 150 loss 0.817809268938785 train RMSE 2.0698948356829194 Validation RMSE 150 : 2.0890241590049037 max of weights 0.2248174078912358\n",
      "Iteration 160 loss 0.8279770542899278 train RMSE 2.0956176245290243 Validation RMSE 160 : 2.102948883086044 max of weights 0.23975751251020905\n",
      "Iteration 170 loss 0.8433899345105657 train RMSE 2.134653370554137 Validation RMSE 170 : 2.122979844283349 max of weights 0.2442816388104763\n",
      "Iteration 180 loss 0.8390912733106733 train RMSE 2.1237613064875873 Validation RMSE 180 : 2.113281678963477 max of weights 0.2483059847484696\n",
      "Iteration 190 loss 0.8299508903547484 train RMSE 2.1005848276146133 Validation RMSE 190 : 2.084265375469559 max of weights 0.2442140660349148\n",
      "Iteration 200 loss 0.8188402088468055 train RMSE 2.0724317059238886 Validation RMSE 200 : 2.0647198654538603 max of weights 0.2422411223236011\n",
      "Iteration 210 loss 0.812809885839762 train RMSE 2.0571482906430547 Validation RMSE 210 : 2.0686883314869347 max of weights 0.24778139696866458\n",
      "Iteration 220 loss 0.815316028099202 train RMSE 2.063474784736981 Validation RMSE 220 : 2.0702937030177915 max of weights 0.2520818825319921\n",
      "Iteration 230 loss 0.8135305233621107 train RMSE 2.0589090715677485 Validation RMSE 230 : 2.0741762085126574 max of weights 0.24468558997171924\n",
      "Iteration 240 loss 0.8074245006270941 train RMSE 2.0433964118214853 Validation RMSE 240 : 2.0725132448750436 max of weights 0.24764288403277218\n",
      "Iteration 250 loss 0.7995955205984185 train RMSE 2.0235263768087055 Validation RMSE 250 : 2.0710007575973557 max of weights 0.24522656009720303\n",
      "Iteration 260 loss 0.7946145541149169 train RMSE 2.010910403169697 Validation RMSE 260 : 2.0636528878911444 max of weights 0.24301974917017943\n",
      "Iteration 270 loss 0.8050278513861455 train RMSE 2.0372657059005013 Validation RMSE 270 : 2.0726099706260848 max of weights 0.24666568732788305\n",
      "Iteration 280 loss 0.8104546364322623 train RMSE 2.0509828806497015 Validation RMSE 280 : 2.0807864820325217 max of weights 0.25313664474046926\n",
      "Iteration 290 loss 0.8119763064149337 train RMSE 2.0548078094136066 Validation RMSE 290 : 2.086099636389974 max of weights 0.2576396649301939\n",
      "Iteration 300 loss 0.8094550662680686 train RMSE 2.048382552214549 Validation RMSE 300 : 2.0823772578010553 max of weights 0.26109963311318324\n",
      "Iteration 310 loss 0.8152834327049231 train RMSE 2.063134550188585 Validation RMSE 310 : 2.0707373924791637 max of weights 0.26531395195511787\n",
      "Iteration 320 loss 0.8222270471685206 train RMSE 2.0806826131667937 Validation RMSE 320 : 2.0707896560863532 max of weights 0.26885863076155636\n",
      "Iteration 330 loss 0.8188836681669983 train RMSE 2.072163070400698 Validation RMSE 330 : 2.064517299287274 max of weights 0.27177076459242266\n",
      "Iteration 340 loss 0.8160194623040482 train RMSE 2.0648723592779787 Validation RMSE 340 : 2.068946827695067 max of weights 0.27201658802443035\n",
      "Iteration 350 loss 0.813576748238596 train RMSE 2.05865902383449 Validation RMSE 350 : 2.0810541147784845 max of weights 0.28511441369671886\n",
      "Iteration 360 loss 0.8280273919813381 train RMSE 2.0952205059009694 Validation RMSE 360 : 2.1065288202824717 max of weights 0.2911258242126366\n",
      "Iteration 370 loss 0.8407389003135871 train RMSE 2.1274175239402173 Validation RMSE 370 : 2.122604890398056 max of weights 0.28592680437964996\n",
      "Iteration 380 loss 0.8304034658081196 train RMSE 2.101241720776092 Validation RMSE 380 : 2.0969425717998242 max of weights 0.2833553192874716\n",
      "Iteration 390 loss 0.8180688586668672 train RMSE 2.0699775260792803 Validation RMSE 390 : 2.0610015709471727 max of weights 0.2785914532918311\n",
      "Iteration 400 loss 0.8044985341118873 train RMSE 2.03559455374901 Validation RMSE 400 : 2.0477491529245477 max of weights 0.2807350691118138\n",
      "Iteration 410 loss 0.8027245575896083 train RMSE 2.0310870063857593 Validation RMSE 410 : 2.0609446380807106 max of weights 0.28962405290936505\n",
      "Iteration 420 loss 0.8064200054399828 train RMSE 2.0404126400190883 Validation RMSE 420 : 2.0721631607904167 max of weights 0.29101824857472613\n",
      "Iteration 430 loss 0.8017666708364162 train RMSE 2.02857892096205 Validation RMSE 430 : 2.073072152823965 max of weights 0.2849011626375278\n",
      "Iteration 440 loss 0.7924417367460005 train RMSE 2.004905524040458 Validation RMSE 440 : 2.0692291774123737 max of weights 0.2942532480179589\n",
      "Iteration 450 loss 0.7853266243448794 train RMSE 1.9868498257689424 Validation RMSE 450 : 2.069233941161194 max of weights 0.2981835429266892\n",
      "Iteration 460 loss 0.782303958489652 train RMSE 1.9791875894713573 Validation RMSE 460 : 2.0617315662184126 max of weights 0.30156344063472557\n",
      "Iteration 470 loss 0.7958968544309617 train RMSE 2.0135852025467735 Validation RMSE 470 : 2.0745089227022615 max of weights 0.30477314724474575\n",
      "Iteration 480 loss 0.8014599104858237 train RMSE 2.0276488376395423 Validation RMSE 480 : 2.082960418355027 max of weights 0.3118147327128707\n",
      "Iteration 490 loss 0.8069881211064007 train RMSE 2.0416091974716695 Validation RMSE 490 : 2.0897347695204944 max of weights 0.3243331706833336\n",
      "Iteration 500 loss 0.805329162589593 train RMSE 2.037370836986028 Validation RMSE 500 : 2.0822281000644343 max of weights 0.33310516359468745\n",
      "Iteration 510 loss 0.8131122755263235 train RMSE 2.057058617931658 Validation RMSE 510 : 2.0672559049986354 max of weights 0.33766210941707403\n",
      "Iteration 520 loss 0.8196457481793264 train RMSE 2.073557961640143 Validation RMSE 520 : 2.063034372244024 max of weights 0.3404135213544402\n",
      "Iteration 530 loss 0.8142930370408774 train RMSE 2.0599521769244444 Validation RMSE 530 : 2.060851409462698 max of weights 0.3371542772050243\n",
      "Iteration 540 loss 0.8088112937172187 train RMSE 2.046036253491259 Validation RMSE 540 : 2.062114668341723 max of weights 0.3401291903578442\n",
      "Iteration 550 loss 0.8098743078208042 train RMSE 2.048702056499363 Validation RMSE 550 : 2.0793647780584608 max of weights 0.34494607672319966\n",
      "Iteration 560 loss 0.8273500181996234 train RMSE 2.0929221415104666 Validation RMSE 560 : 2.1148114493131898 max of weights 0.3443535258741476\n",
      "Iteration 570 loss 0.8328647303633173 train RMSE 2.1068889167412634 Validation RMSE 570 : 2.1168267509775207 max of weights 0.34170019718508493\n",
      "Iteration 580 loss 0.8167806670441539 train RMSE 2.0661528613529567 Validation RMSE 580 : 2.0765968306684672 max of weights 0.3431284839576039\n",
      "Iteration 590 loss 0.8053714878755939 train RMSE 2.0372360975000174 Validation RMSE 590 : 2.040635498966459 max of weights 0.34504685797581686\n",
      "Iteration 600 loss 0.7903690197714607 train RMSE 1.999225163643611 Validation RMSE 600 : 2.0397276660058865 max of weights 0.350014956380202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 610 loss 0.7936468229372441 train RMSE 2.0075077730175317 Validation RMSE 610 : 2.057941103562591 max of weights 0.35430627038374407\n",
      "Iteration 620 loss 0.7984533760710435 train RMSE 2.0196418663839326 Validation RMSE 620 : 2.0735598670118005 max of weights 0.35042787262506037\n",
      "Iteration 630 loss 0.7885608730289505 train RMSE 1.9945427031387322 Validation RMSE 630 : 2.0682977902693076 max of weights 0.343881935661157\n",
      "Iteration 640 loss 0.7783651903182461 train RMSE 1.968664297171954 Validation RMSE 640 : 2.0663816850928995 max of weights 0.34882295665066954\n",
      "Iteration 650 loss 0.7723875693934842 train RMSE 1.9534994711541878 Validation RMSE 650 : 2.0654812922348142 max of weights 0.3566732318475272\n",
      "Iteration 660 loss 0.7768495146059214 train RMSE 1.9647936665598855 Validation RMSE 660 : 2.062648357678569 max of weights 0.3616312021603846\n",
      "Iteration 670 loss 0.7908215083586304 train RMSE 2.000149594377394 Validation RMSE 670 : 2.079394086776996 max of weights 0.37158096409915525\n",
      "Iteration 680 loss 0.7945003696595566 train RMSE 2.0094418110207255 Validation RMSE 680 : 2.084132160272043 max of weights 0.38113479340157624\n",
      "Iteration 690 loss 0.8026258463956338 train RMSE 2.0299777198912996 Validation RMSE 690 : 2.095473495369819 max of weights 0.3894850588325365\n",
      "Iteration 700 loss 0.8023458751354866 train RMSE 2.0292394311426265 Validation RMSE 700 : 2.080221387481306 max of weights 0.3944267921739917\n",
      "Iteration 710 loss 0.8122010274642855 train RMSE 2.0541668545639835 Validation RMSE 710 : 2.070263758242682 max of weights 0.3980534088282516\n",
      "Iteration 720 loss 0.8176231170851542 train RMSE 2.067846016228099 Validation RMSE 720 : 2.0625758957836084 max of weights 0.3952729808134603\n",
      "Iteration 730 loss 0.8096169781728085 train RMSE 2.0475312477742027 Validation RMSE 730 : 2.0645779750425692 max of weights 0.3909231604790867\n",
      "Iteration 740 loss 0.8031341427345252 train RMSE 2.03108245367101 Validation RMSE 740 : 2.065431159169034 max of weights 0.3944123540392109\n",
      "Iteration 750 loss 0.807905880693019 train RMSE 2.0431399887391866 Validation RMSE 750 : 2.0848179784978553 max of weights 0.3965383399977637\n",
      "Iteration 760 loss 0.8269721914901476 train RMSE 2.0913924609936525 Validation RMSE 760 : 2.12158840262617 max of weights 0.3945753136936852\n",
      "Iteration 770 loss 0.8246905707305071 train RMSE 2.085613134223208 Validation RMSE 770 : 2.1054855433880717 max of weights 0.39199033722338766\n",
      "Iteration 780 loss 0.8084740760418606 train RMSE 2.0445373761940915 Validation RMSE 780 : 2.0608374419843782 max of weights 0.3907353279459776\n",
      "Iteration 790 loss 0.7984607427143425 train RMSE 2.0191589744282803 Validation RMSE 790 : 2.032601610925197 max of weights 0.3920095985059419\n",
      "Iteration 800 loss 0.7829066355237521 train RMSE 1.9797539521527312 Validation RMSE 800 : 2.045707285054339 max of weights 0.3966170122359148\n",
      "Iteration 810 loss 0.7912589273501586 train RMSE 2.0008804343396105 Validation RMSE 810 : 2.063734058643806 max of weights 0.3996592664775096\n",
      "Iteration 820 loss 0.7933779315476289 train RMSE 2.006205710718177 Validation RMSE 820 : 2.0776180002029423 max of weights 0.3932145695555222\n",
      "Iteration 830 loss 0.7816847919875701 train RMSE 1.9765517107738668 Validation RMSE 830 : 2.0698530477494366 max of weights 0.38786229961123714\n",
      "Iteration 840 loss 0.7714252276680691 train RMSE 1.9505156615515378 Validation RMSE 840 : 2.0720096154744017 max of weights 0.3937975180396546\n",
      "Iteration 850 loss 0.764741359499109 train RMSE 1.9335713773749703 Validation RMSE 850 : 2.064808605457301 max of weights 0.40057859714522975\n",
      "Iteration 860 loss 0.77488481905537 train RMSE 1.9592504335545593 Validation RMSE 860 : 2.0715392747956423 max of weights 0.40452849102200544\n",
      "Iteration 870 loss 0.7868309708727147 train RMSE 1.9894786572758805 Validation RMSE 870 : 2.0876736923263213 max of weights 0.41392825528410876\n",
      "Iteration 880 loss 0.7890021370235015 train RMSE 1.9949571534386512 Validation RMSE 880 : 2.091342829580371 max of weights 0.4250346881992221\n",
      "Iteration 890 loss 0.7973501295133868 train RMSE 2.0160551855639195 Validation RMSE 890 : 2.103850796874804 \n",
      "Performance (RMSE) on PCE:\n",
      "Train: 2.2850334432016624\n",
      "Test:  2.0830770322561856\n",
      "--------------------------------------------------------------------------------\n",
      "Starting neural fingerprint experiment...\n",
      "Total number of weights in the network: 41771\n",
      "max of weights 0.08535001578936458\n",
      "Iteration 0 loss 0.9413061884248309 train RMSE 2.383355901749104 Validation RMSE 0 : 2.3532523067537956 max of weights 0.08624429518314863\n",
      "Iteration 10 loss 0.9324823255871271 train RMSE 2.361004803808509 Validation RMSE 10 : 2.1887688889103187 max of weights 0.10644619081032873\n",
      "Iteration 20 loss 0.9285165874590178 train RMSE 2.350948576224173 Validation RMSE 20 : 2.230492768818653 max of weights 0.13613034381883912\n",
      "Iteration 30 loss 0.9057152881029209 train RMSE 2.2930250092569286 Validation RMSE 30 : 2.127706403770575 max of weights 0.16783200811575735\n",
      "Iteration 40 loss 0.7959373265877059 train RMSE 2.01455433770379 Validation RMSE 40 : 1.6605224450050016 max of weights 0.18351709209254952\n",
      "Iteration 50 loss 0.7129618989386506 train RMSE 1.8040867006944084 Validation RMSE 50 : 1.468629469979089 max of weights 0.18771802476985136\n",
      "Iteration 60 loss 0.7129516136549194 train RMSE 1.8040009634425394 Validation RMSE 60 : 1.4437136336867031 max of weights 0.20696120660783524\n",
      "Iteration 70 loss 0.7064391619339141 train RMSE 1.7872909499361804 Validation RMSE 70 : 1.3729652533929313 max of weights 0.2206506611396322\n",
      "Iteration 80 loss 0.6742770310066453 train RMSE 1.7057553829765482 Validation RMSE 80 : 1.3323570838403036 max of weights 0.23800291606750712\n",
      "Iteration 90 loss 0.6444856113953132 train RMSE 1.6302443535990734 Validation RMSE 90 : 1.3359572499991248 max of weights 0.24255938548914383\n",
      "Iteration 100 loss 0.6468693271005322 train RMSE 1.6361362521263967 Validation RMSE 100 : 1.257531177113704 max of weights 0.23595949659779722\n",
      "Iteration 110 loss 0.667975469536029 train RMSE 1.6896151374012671 Validation RMSE 110 : 1.3509903906937637 max of weights 0.23661030827851548\n",
      "Iteration 120 loss 0.6444471747458639 train RMSE 1.6299358275755862 Validation RMSE 120 : 1.3133245153896351 max of weights 0.2432528244575085\n",
      "Iteration 130 loss 0.6394680761729247 train RMSE 1.6172558694222403 Validation RMSE 130 : 1.2894971559695867 max of weights 0.26031585939485774\n",
      "Iteration 140 loss 0.6569703587794239 train RMSE 1.66151936654393 Validation RMSE 140 : 1.3575180323402631 max of weights 0.2735987948789698\n",
      "Iteration 150 loss 0.636113687638243 train RMSE 1.608683316344631 Validation RMSE 150 : 1.3113281703331727 max of weights 0.2813411171879752\n",
      "Iteration 160 loss 0.6860043257187688 train RMSE 1.7349421751057688 Validation RMSE 160 : 1.3403571433806978 max of weights 0.29481514360841105\n",
      "Iteration 170 loss 0.6815211906320586 train RMSE 1.7236217343033664 Validation RMSE 170 : 1.4478813623819227 max of weights 0.31036468463308087\n",
      "Iteration 180 loss 0.6329487027128398 train RMSE 1.6005558052269178 Validation RMSE 180 : 1.3290657002367277 max of weights 0.3149047719669828\n",
      "Iteration 190 loss 0.6393295540657006 train RMSE 1.6167525854440248 Validation RMSE 190 : 1.3653587781336958 max of weights 0.3290657120458418\n",
      "Iteration 200 loss 0.623183471273097 train RMSE 1.5758225829287569 Validation RMSE 200 : 1.3271366630043788 max of weights 0.341457624889849\n",
      "Iteration 210 loss 0.6292632416355871 train RMSE 1.5911793747804084 Validation RMSE 210 : 1.3706160320222678 max of weights 0.35658207125719427\n",
      "Iteration 220 loss 0.6493220051524675 train RMSE 1.6419028825897322 Validation RMSE 220 : 1.3736640066946504 max of weights 0.3669771422788625\n",
      "Iteration 230 loss 0.62021013864561 train RMSE 1.568127593799395 Validation RMSE 230 : 1.2607920353641857 max of weights 0.3806357553178877\n",
      "Iteration 240 loss 0.6165121640237368 train RMSE 1.5587960673230834 Validation RMSE 240 : 1.3610381909987677 max of weights 0.398160871635772\n",
      "Iteration 250 loss 0.6239259918796412 train RMSE 1.5775194693002297 Validation RMSE 250 : 1.3761645788585843 max of weights 0.41216545193054016\n",
      "Iteration 260 loss 0.6450165670464153 train RMSE 1.6309263952681772 Validation RMSE 260 : 1.3883679938562783 max of weights 0.4258484292687877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 270 loss 0.6269020807459134 train RMSE 1.5849836693245236 Validation RMSE 270 : 1.315641751041517 max of weights 0.4399137455189865\n",
      "Iteration 280 loss 0.6279361234405393 train RMSE 1.5876054777068718 Validation RMSE 280 : 1.3350412226419421 max of weights 0.46149638158946704\n",
      "Iteration 290 loss 0.6193940875009991 train RMSE 1.565950083237545 Validation RMSE 290 : 1.349455616240558 max of weights 0.469041011566497\n",
      "Iteration 300 loss 0.6404061861305668 train RMSE 1.619097061700056 Validation RMSE 300 : 1.2795156593651718 max of weights 0.4868191313175739\n",
      "Iteration 310 loss 0.6421527264390964 train RMSE 1.6235640949950436 Validation RMSE 310 : 1.378492630203372 max of weights 0.5089146186937037\n",
      "Iteration 320 loss 0.657551259493034 train RMSE 1.6624647429091761 Validation RMSE 320 : 1.3578713336536172 max of weights 0.5277828525667964\n",
      "Iteration 330 loss 0.6287664089874075 train RMSE 1.5895723346756039 Validation RMSE 330 : 1.350203655621367 max of weights 0.5427490688882388\n",
      "Iteration 340 loss 0.6425650572022571 train RMSE 1.624443904914876 Validation RMSE 340 : 1.3864346881809588 max of weights 0.5552202431457893\n",
      "Iteration 350 loss 0.6300253398775927 train RMSE 1.5927448195968885 Validation RMSE 350 : 1.3952898049414455 max of weights 0.5586492574558826\n",
      "Iteration 360 loss 0.6821655509970166 train RMSE 1.7246765325500955 Validation RMSE 360 : 1.3930723772474198 max of weights 0.5781612388703358\n",
      "Iteration 370 loss 0.6591852586414254 train RMSE 1.6665570453618586 Validation RMSE 370 : 1.4445552179837664 max of weights 0.5933967131292814\n",
      "Iteration 380 loss 0.6285276584210334 train RMSE 1.5888818279115022 Validation RMSE 380 : 1.3204875150141622 max of weights 0.6058183743800745\n",
      "Iteration 390 loss 0.640256314328494 train RMSE 1.618614284464725 Validation RMSE 390 : 1.4061923184367826 max of weights 0.6246708123226494\n",
      "Iteration 400 loss 0.611174241145028 train RMSE 1.544925712283144 Validation RMSE 400 : 1.3404484306355948 max of weights 0.6396066881032109\n",
      "Iteration 410 loss 0.6338009422792386 train RMSE 1.602206169434317 Validation RMSE 410 : 1.4198587732051016 max of weights 0.6543498713063802\n",
      "Iteration 420 loss 0.6408423993993635 train RMSE 1.6199543745622043 Validation RMSE 420 : 1.376270783982723 max of weights 0.6627989481107693\n",
      "Iteration 430 loss 0.6144714303614583 train RMSE 1.553156044212985 Validation RMSE 430 : 1.2711260448246533 max of weights 0.6826763587046116\n",
      "Iteration 440 loss 0.6121713235628853 train RMSE 1.5473411329282283 Validation RMSE 440 : 1.375816568271722 max of weights 0.7035935145082604\n",
      "Iteration 450 loss 0.6299313920178211 train RMSE 1.5922621262750627 Validation RMSE 450 : 1.4054069580415143 max of weights 0.7233446815605082\n",
      "Iteration 460 loss 0.6379533589126704 train RMSE 1.612598270126271 Validation RMSE 460 : 1.4469818534410372 max of weights 0.736299675385672\n",
      "Iteration 470 loss 0.6177169166807395 train RMSE 1.5612839727483239 Validation RMSE 470 : 1.3210786021419774 max of weights 0.7552366423553095\n",
      "Iteration 480 loss 0.6244076552451973 train RMSE 1.5782371997148124 Validation RMSE 480 : 1.3689815615217271 max of weights 0.7815395263012566\n",
      "Iteration 490 loss 0.6221482181082134 train RMSE 1.57247502133133 Validation RMSE 490 : 1.3797447196803203 max of weights 0.7925521859219351\n",
      "Iteration 500 loss 0.6302748549168485 train RMSE 1.5930149587389335 Validation RMSE 500 : 1.302976770320758 max of weights 0.8146846386372818\n",
      "Iteration 510 loss 0.6436996697482344 train RMSE 1.627023491648257 Validation RMSE 510 : 1.4313768655056551 max of weights 0.8364140063308215\n",
      "Iteration 520 loss 0.655849153751611 train RMSE 1.6576978504306472 Validation RMSE 520 : 1.3793404286172797 max of weights 0.8558329268857923\n",
      "Iteration 530 loss 0.6348999712381046 train RMSE 1.6046420951318705 Validation RMSE 530 : 1.3733786167276267 max of weights 0.8759354648763172\n",
      "Iteration 540 loss 0.6412501540085674 train RMSE 1.6206621321748547 Validation RMSE 540 : 1.4169133113518866 max of weights 0.883788136728657\n",
      "Iteration 550 loss 0.633078415221025 train RMSE 1.6000234299677585 Validation RMSE 550 : 1.4365677726286885 max of weights 0.8859853035251577\n",
      "Iteration 560 loss 0.6715976728005344 train RMSE 1.6974921014166027 Validation RMSE 560 : 1.4040679915990009 max of weights 0.9092701347725289\n",
      "Iteration 570 loss 0.6439227333603408 train RMSE 1.6274733134859254 Validation RMSE 570 : 1.4362691094038968 max of weights 0.9217876268232572\n",
      "Iteration 580 loss 0.6147726716533569 train RMSE 1.5536358843563773 Validation RMSE 580 : 1.3240815876528471 max of weights 0.9376369071178287\n",
      "Iteration 590 loss 0.6165065110791083 train RMSE 1.5580475648137349 Validation RMSE 590 : 1.3898234041902124 max of weights 0.9558476018870886\n",
      "Iteration 600 loss 0.6037286500118075 train RMSE 1.5256244194498243 Validation RMSE 600 : 1.3433506116310916 max of weights 0.9713519409314341\n",
      "Iteration 610 loss 0.6355703412843263 train RMSE 1.6062385440967923 Validation RMSE 610 : 1.4391380030604992 max of weights 0.9850993599259261\n",
      "Iteration 620 loss 0.6360299034200771 train RMSE 1.6073172550770651 Validation RMSE 620 : 1.366164927439172 max of weights 0.9927213575387036\n",
      "Iteration 630 loss 0.6018477317734452 train RMSE 1.5207879503056256 Validation RMSE 630 : 1.3036025009652594 max of weights 1.0165576759966946\n",
      "Iteration 640 loss 0.6030886163176218 train RMSE 1.5239207546607545 Validation RMSE 640 : 1.381101552906013 max of weights 1.0357666907694827\n",
      "Iteration 650 loss 0.6207210930373851 train RMSE 1.5685127936641707 Validation RMSE 650 : 1.4118124454072065 max of weights 1.0533708180714698\n",
      "Iteration 660 loss 0.6303805145906776 train RMSE 1.5929913690052957 Validation RMSE 660 : 1.4626880515131355 max of weights 1.064038826118443\n",
      "Iteration 670 loss 0.6062925507753293 train RMSE 1.5319388454411518 Validation RMSE 670 : 1.3356853889925664 max of weights 1.0869694496053801\n",
      "Iteration 680 loss 0.6125195074591109 train RMSE 1.5476941098165966 Validation RMSE 680 : 1.3781069019945782 max of weights 1.1101187315037593\n",
      "Iteration 690 loss 0.6127199322732249 train RMSE 1.5481510254695772 Validation RMSE 690 : 1.3780092469592857 max of weights 1.122040760906569\n",
      "Iteration 700 loss 0.6215467211780166 train RMSE 1.5704811469771076 Validation RMSE 700 : 1.353290375557258 max of weights 1.1466728424708998\n",
      "Iteration 710 loss 0.6272112201172684 train RMSE 1.5847676540239155 Validation RMSE 710 : 1.4321696202736693 max of weights 1.1619268227363977\n",
      "Iteration 720 loss 0.6361439594055674 train RMSE 1.60730432853524 Validation RMSE 720 : 1.3730089799512637 max of weights 1.178447063574636\n",
      "Iteration 730 loss 0.6194696361883253 train RMSE 1.565078505308213 Validation RMSE 730 : 1.4224733666666523 max of weights 1.190200864906296\n",
      "Iteration 740 loss 0.6116744800714446 train RMSE 1.5452735255780508 Validation RMSE 740 : 1.378100789365639 max of weights 1.1970685846421947\n",
      "Iteration 750 loss 0.6279953899689477 train RMSE 1.5866284702593867 Validation RMSE 750 : 1.4974568270166357 max of weights 1.1935504957342116\n",
      "Iteration 760 loss 0.6431618908945622 train RMSE 1.6249849245736148 Validation RMSE 760 : 1.4104047245020663 max of weights 1.2143052648431834\n",
      "Iteration 770 loss 0.6035653538090818 train RMSE 1.5247344364437876 Validation RMSE 770 : 1.3715297510659095 max of weights 1.2251966190948187\n",
      "Iteration 780 loss 0.598272063321408 train RMSE 1.5113069371143588 Validation RMSE 780 : 1.3748965877663677 max of weights 1.2406656542538383\n",
      "Iteration 790 loss 0.5861575548502181 train RMSE 1.4806293291423842 Validation RMSE 790 : 1.385399782396229 max of weights 1.252342112347614\n",
      "Iteration 800 loss 0.5939201755398799 train RMSE 1.500232517740136 Validation RMSE 800 : 1.3630528631476417 max of weights 1.2680029276148\n",
      "Iteration 810 loss 0.6163908344362959 train RMSE 1.5571027388283867 Validation RMSE 810 : 1.4377219900086797 max of weights 1.2761910332712871\n",
      "Iteration 820 loss 0.6145907637880619 train RMSE 1.5524947798177768 Validation RMSE 820 : 1.3536410038160178 max of weights 1.2837124334181707\n",
      "Iteration 830 loss 0.5811256914653316 train RMSE 1.467769784753993 Validation RMSE 830 : 1.3637772237131178 max of weights 1.3065901013012295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 840 loss 0.5927301887628121 train RMSE 1.4971068537419625 Validation RMSE 840 : 1.4317073092857386 max of weights 1.3173575915439493\n",
      "Iteration 850 loss 0.5850127762203887 train RMSE 1.4775485927852683 Validation RMSE 850 : 1.411391041551979 max of weights 1.323692778810133\n",
      "Iteration 860 loss 0.6128828003584331 train RMSE 1.548109651034858 Validation RMSE 860 : 1.493185303198071 max of weights 1.3308902763500237\n",
      "Iteration 870 loss 0.5828216160150949 train RMSE 1.4719710080488257 Validation RMSE 870 : 1.361045387898216 max of weights 1.3496007478019791\n",
      "Iteration 880 loss 0.5876770913121582 train RMSE 1.4842265922954168 Validation RMSE 880 : 1.40577851583553 max of weights 1.3546092784933141\n",
      "Iteration 890 loss 0.5820578098932715 train RMSE 1.4699499774958606 Validation RMSE 890 : 1.3344720231956197\n",
      "Morgan test RMSE: 2.0830770322561856 Neural test RMSE: 1.6761424673542065\n"
     ]
    }
   ],
   "source": [
    "# Example regression script using neural fingerprints.\n",
    "#\n",
    "# Compares Morgan fingerprints to neural fingerprints.\n",
    "\n",
    "import autograd.numpy as np\n",
    "import autograd.numpy.random as npr\n",
    "\n",
    "from neuralfingerprint import load_data\n",
    "from neuralfingerprint import build_morgan_deep_net\n",
    "from neuralfingerprint import build_conv_deep_net\n",
    "from neuralfingerprint import normalize_array, adam\n",
    "from neuralfingerprint import build_batched_grad\n",
    "from neuralfingerprint.util import rmse\n",
    "\n",
    "from autograd import grad\n",
    "\n",
    "task_params = {'target_name' : 'PCE',\n",
    "               'data_file'   : 'cep-processed.csv'}\n",
    "N_train = 19800\n",
    "N_val   = 100\n",
    "N_test  = 100\n",
    "\n",
    "model_params = dict(fp_length=50,    # Usually neural fps need far fewer dimensions than morgan.\n",
    "                    fp_depth=6,      # The depth of the network equals the fingerprint radius.\n",
    "                    conv_width=20,   # Only the neural fps need this parameter.\n",
    "                    h1_size=100,     # Size of hidden layer of network on top of fps.\n",
    "                    L2_reg=np.exp(-2))\n",
    "train_params = dict(num_iters=900,\n",
    "                    batch_size=100,\n",
    "                    init_scale=np.exp(-4),\n",
    "                    step_size=np.exp(-6))\n",
    "\n",
    "# Define the architecture of the network that sits on top of the fingerprints.\n",
    "vanilla_net_params = dict(\n",
    "    layer_sizes = [model_params['fp_length'], model_params['h1_size']],  # One hidden layer.\n",
    "    normalize=True, L2_reg = model_params['L2_reg'], nll_func = rmse)\n",
    "\n",
    "def train_nn(pred_fun, loss_fun, num_weights, train_smiles, train_raw_targets, train_params, seed=0,\n",
    "             validation_smiles=None, validation_raw_targets=None):\n",
    "    \"\"\"loss_fun has inputs (weights, smiles, targets)\"\"\"\n",
    "    print \"Total number of weights in the network:\", num_weights\n",
    "    init_weights = npr.RandomState(seed).randn(num_weights) * train_params['init_scale']\n",
    "\n",
    "    num_print_examples = 100\n",
    "    train_targets, undo_norm = normalize_array(train_raw_targets)\n",
    "    training_curve = []\n",
    "    def callback(weights, iter):\n",
    "        if iter % 10 == 0:\n",
    "            print \"max of weights\", np.max(np.abs(weights))\n",
    "            train_preds = undo_norm(pred_fun(weights, train_smiles[:num_print_examples]))\n",
    "            cur_loss = loss_fun(weights, train_smiles[:num_print_examples], train_targets[:num_print_examples])\n",
    "            training_curve.append(cur_loss)\n",
    "            print \"Iteration\", iter, \"loss\", cur_loss,\\\n",
    "                  \"train RMSE\", rmse(train_preds, train_raw_targets[:num_print_examples]),\n",
    "            if validation_smiles is not None:\n",
    "                validation_preds = undo_norm(pred_fun(weights, validation_smiles))\n",
    "                print \"Validation RMSE\", iter, \":\", rmse(validation_preds, validation_raw_targets),\n",
    "\n",
    "    # Build gradient using autograd.\n",
    "    grad_fun = grad(loss_fun)\n",
    "    grad_fun_with_data = build_batched_grad(grad_fun, train_params['batch_size'],\n",
    "                                            train_smiles, train_targets)\n",
    "\n",
    "    # Optimize weights.\n",
    "    trained_weights = adam(grad_fun_with_data, init_weights, callback=callback,\n",
    "                           num_iters=train_params['num_iters'], step_size=train_params['step_size'])\n",
    "\n",
    "    def predict_func(new_smiles):\n",
    "        \"\"\"Returns to the original units that the raw targets were in.\"\"\"\n",
    "        return undo_norm(pred_fun(trained_weights, new_smiles))\n",
    "    return predict_func, trained_weights, training_curve\n",
    "\n",
    "\n",
    "def main():\n",
    "    print \"Loading data...\"\n",
    "    traindata, valdata, testdata = load_data(\n",
    "        task_params['data_file'], (N_train, N_val, N_test),\n",
    "        input_name='smiles', target_name=task_params['target_name'])\n",
    "    train_inputs, train_targets = traindata\n",
    "    val_inputs,   val_targets   = valdata\n",
    "    test_inputs,  test_targets  = testdata\n",
    "\n",
    "    def print_performance(pred_func):\n",
    "        train_preds = pred_func(train_inputs)\n",
    "        val_preds = pred_func(val_inputs)\n",
    "        print \"\\nPerformance (RMSE) on \" + task_params['target_name'] + \":\"\n",
    "        print \"Train:\", rmse(train_preds, train_targets)\n",
    "        print \"Test: \", rmse(val_preds,  val_targets)\n",
    "        print \"-\" * 80\n",
    "        return rmse(val_preds, val_targets)\n",
    "\n",
    "    def run_morgan_experiment():\n",
    "        loss_fun, pred_fun, net_parser = \\\n",
    "            build_morgan_deep_net(model_params['fp_length'],\n",
    "                                  model_params['fp_depth'], vanilla_net_params)\n",
    "        num_weights = len(net_parser)\n",
    "        predict_func, trained_weights, conv_training_curve = \\\n",
    "            train_nn(pred_fun, loss_fun, num_weights, train_inputs, train_targets,\n",
    "                     train_params, validation_smiles=val_inputs, validation_raw_targets=val_targets)\n",
    "        return print_performance(predict_func)\n",
    "\n",
    "    def run_conv_experiment():\n",
    "        conv_layer_sizes = [model_params['conv_width']] * model_params['fp_depth']\n",
    "        conv_arch_params = {'num_hidden_features' : conv_layer_sizes,\n",
    "                            'fp_length' : model_params['fp_length'], 'normalize' : 1}\n",
    "        loss_fun, pred_fun, conv_parser = \\\n",
    "            build_conv_deep_net(conv_arch_params, vanilla_net_params, model_params['L2_reg'])\n",
    "        num_weights = len(conv_parser)\n",
    "        predict_func, trained_weights, conv_training_curve = \\\n",
    "            train_nn(pred_fun, loss_fun, num_weights, train_inputs, train_targets,\n",
    "                     train_params, validation_smiles=val_inputs, validation_raw_targets=val_targets)\n",
    "        test_predictions = predict_func(test_inputs)\n",
    "        return rmse(test_predictions, test_targets)\n",
    "\n",
    "    print \"Task params\", task_params\n",
    "    print\n",
    "    print \"Starting Morgan fingerprint experiment...\"\n",
    "    test_loss_morgan = run_morgan_experiment()\n",
    "    print \"Starting neural fingerprint experiment...\"\n",
    "    test_loss_neural = run_conv_experiment()\n",
    "    print\n",
    "    print \"Morgan test RMSE:\", test_loss_morgan, \"Neural test RMSE:\", test_loss_neural\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Fingerprint drug efficacy result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Task params {'target_name': 'activity', 'data_file': 'malaria-processed.csv'}\n",
      "\n",
      "Starting Morgan fingerprint experiment...\n",
      "Total number of weights in the network: 5201\n",
      "max of weights 0.06962983567500523\n",
      "Iteration 0 loss 0.819681257416317 train RMSE 0.9969002045258831 Validation RMSE 0 : 1.2043149332577938 max of weights 0.07764954491251569\n",
      "Iteration 10 loss 0.8092990967412906 train RMSE 0.9842587942540298 Validation RMSE 10 : 1.197878260767718 max of weights 0.0826609586957519\n",
      "Iteration 20 loss 0.8056825758001497 train RMSE 0.9798403579576411 Validation RMSE 20 : 1.1962065000125355 max of weights 0.09842937474201935\n",
      "Iteration 30 loss 0.7986581402212991 train RMSE 0.9712692654347401 Validation RMSE 30 : 1.1935355786123156 max of weights 0.10905974230467713\n",
      "Iteration 40 loss 0.792582788066391 train RMSE 0.9638492276613394 Validation RMSE 40 : 1.194161178458829 max of weights 0.12172841950819184\n",
      "Iteration 50 loss 0.7889750401098654 train RMSE 0.9594404320765614 Validation RMSE 50 : 1.1985849005152094 max of weights 0.13224110945553202\n",
      "Iteration 60 loss 0.788275747968028 train RMSE 0.9585788325257155 Validation RMSE 60 : 1.1961933258251767 max of weights 0.1428723629978221\n",
      "Iteration 70 loss 0.7884349438494656 train RMSE 0.9587592296492699 Validation RMSE 70 : 1.1924540563634893 max of weights 0.14997794718922128\n",
      "Iteration 80 loss 0.7876850940131296 train RMSE 0.9578239266800919 Validation RMSE 80 : 1.1824430099229282 max of weights 0.15814261043199707\n",
      "Iteration 90 loss 0.7871741493113508 train RMSE 0.9571896445035897 Validation RMSE 90 : 1.1744097996732337 max of weights 0.16628777171499814\n",
      "Iteration 100 loss 0.782191032040982 train RMSE 0.9511133137929219 Validation RMSE 100 : 1.1699417710057154 max of weights 0.16957555150595255\n",
      "Iteration 110 loss 0.7761705984750832 train RMSE 0.9437649669866726 Validation RMSE 110 : 1.1701014376771504 max of weights 0.17072985448078393\n",
      "Iteration 120 loss 0.7773827905095471 train RMSE 0.9452186188935573 Validation RMSE 120 : 1.179683711710028 max of weights 0.16439896610266663\n",
      "Iteration 130 loss 0.7782257217470897 train RMSE 0.9462179859038377 Validation RMSE 130 : 1.1831620211936544 max of weights 0.1577053340685031\n",
      "Iteration 140 loss 0.7744526162696688 train RMSE 0.9416178448642607 Validation RMSE 140 : 1.183962532881214 max of weights 0.15153693216206846\n",
      "Iteration 150 loss 0.7739465246398101 train RMSE 0.941000819596642 Validation RMSE 150 : 1.1866524141685375 max of weights 0.15267188884224403\n",
      "Iteration 160 loss 0.7745325839504441 train RMSE 0.9417098120073137 Validation RMSE 160 : 1.1816771614829364 max of weights 0.16458353666063938\n",
      "Iteration 170 loss 0.774780950959112 train RMSE 0.9419956035073657 Validation RMSE 170 : 1.1766749567784707 max of weights 0.17554540116120818\n",
      "Iteration 180 loss 0.7769660167333967 train RMSE 0.9446321257785701 Validation RMSE 180 : 1.1664318850203397 max of weights 0.18171739699192443\n",
      "Iteration 190 loss 0.7768276660055358 train RMSE 0.9444487312449853 Validation RMSE 190 : 1.161163464940084 max of weights 0.18496642378074987\n",
      "Iteration 200 loss 0.770217154499774 train RMSE 0.9363891253548416 Validation RMSE 200 : 1.161839046156373 max of weights 0.19276076104371423\n",
      "Iteration 210 loss 0.768059450355028 train RMSE 0.933740225047956 Validation RMSE 210 : 1.1675980292305765 max of weights 0.1938309852831919\n",
      "Iteration 220 loss 0.7696249118323392 train RMSE 0.9356223791959013 Validation RMSE 220 : 1.175732076647882 max of weights 0.19919297633127397\n",
      "Iteration 230 loss 0.7700469652072306 train RMSE 0.9361094874006132 Validation RMSE 230 : 1.176971175509538 max of weights 0.22204923471009844\n",
      "Iteration 240 loss 0.7677069392499665 train RMSE 0.9332512809896043 Validation RMSE 240 : 1.180287160946216 max of weights 0.2422086214185412\n",
      "Iteration 250 loss 0.7681554954500496 train RMSE 0.9337944594704576 Validation RMSE 250 : 1.1811012546972839 max of weights 0.24962375677211804\n",
      "Iteration 260 loss 0.7688684592057297 train RMSE 0.9346503086774958 Validation RMSE 260 : 1.177728776113153 max of weights 0.26415507853165193\n",
      "Iteration 270 loss 0.7691387238354603 train RMSE 0.9349544081456638 Validation RMSE 270 : 1.1686446005204478 max of weights 0.26809474635414854\n",
      "Iteration 280 loss 0.771603260021193 train RMSE 0.9379321348356625 Validation RMSE 280 : 1.1638123641503917 max of weights 0.27191022741538146\n",
      "Iteration 290 loss 0.7708592844896417 train RMSE 0.9370108514263296 Validation RMSE 290 : 1.1582526951988432 max of weights 0.26780447021886555\n",
      "Iteration 300 loss 0.7610418493594038 train RMSE 0.9250452907018019 Validation RMSE 300 : 1.16051353348006 max of weights 0.2734313748587564\n",
      "Iteration 310 loss 0.7614941461585579 train RMSE 0.9255725368912625 Validation RMSE 310 : 1.1706518033150282 max of weights 0.2772886144671175\n",
      "Iteration 320 loss 0.7634190978191966 train RMSE 0.9278854207544354 Validation RMSE 320 : 1.175853006698795 max of weights 0.29743889867934964\n",
      "Iteration 330 loss 0.762357437530762 train RMSE 0.9265687014291198 Validation RMSE 330 : 1.177037538110328 max of weights 0.32178030576635075\n",
      "Iteration 340 loss 0.762270936849161 train RMSE 0.9264526444896688 Validation RMSE 340 : 1.1837147382060473 max of weights 0.3311481999062406\n",
      "Iteration 350 loss 0.7617671372910307 train RMSE 0.925834775906983 Validation RMSE 350 : 1.1809228871435191 max of weights 0.3361276665371561\n",
      "Iteration 360 loss 0.7626175872437527 train RMSE 0.9268506671329602 Validation RMSE 360 : 1.18010609721332 max of weights 0.3471497963338391\n",
      "Iteration 370 loss 0.7657316939559786 train RMSE 0.9306076687326533 Validation RMSE 370 : 1.167016133156487 max of weights 0.34558145780245186\n",
      "Iteration 380 loss 0.7649238438295193 train RMSE 0.9296063695300175 Validation RMSE 380 : 1.1608614608158558 max of weights 0.3464160365372078\n",
      "Iteration 390 loss 0.7610888859144168 train RMSE 0.9249216305686985 Validation RMSE 390 : 1.1581150844557595 max of weights 0.34101405333694995\n",
      "Iteration 400 loss 0.752515079044212 train RMSE 0.9144660880799838 Validation RMSE 400 : 1.1633513326557934 max of weights 0.3464955731977778\n",
      "Iteration 410 loss 0.755926447287614 train RMSE 0.9185902991865432 Validation RMSE 410 : 1.1760870758532238 max of weights 0.3501857887654823\n",
      "Iteration 420 loss 0.7564561243521954 train RMSE 0.9191996038990385 Validation RMSE 420 : 1.1781403086760043 max of weights 0.368868356004938\n",
      "Iteration 430 loss 0.7536615568858265 train RMSE 0.9157784870284464 Validation RMSE 430 : 1.1804453625945033 max of weights 0.38738367688853914\n",
      "Iteration 440 loss 0.7558731347196531 train RMSE 0.9184591627691158 Validation RMSE 440 : 1.1891561079971196 max of weights 0.3878748537875236\n",
      "Iteration 450 loss 0.7525367451325112 train RMSE 0.9143892426224252 Validation RMSE 450 : 1.182965501636107 max of weights 0.3946331662319298\n",
      "Iteration 460 loss 0.7529152101024498 train RMSE 0.9148261821870172 Validation RMSE 460 : 1.179977886485758 max of weights 0.4007923926922309\n",
      "Iteration 470 loss 0.7580620048621192 train RMSE 0.9210542386051741 Validation RMSE 470 : 1.1678117031352429 max of weights 0.3987436243661642\n",
      "Iteration 480 loss 0.7554591104771611 train RMSE 0.9178680174860621 Validation RMSE 480 : 1.160644465757194 max of weights 0.39590703480126577\n",
      "Iteration 490 loss 0.747797748638818 train RMSE 0.9085241818797298 Validation RMSE 490 : 1.1614339651742849 max of weights 0.3941008790383662\n",
      "Iteration 500 loss 0.7444060754742173 train RMSE 0.9043688944958309 Validation RMSE 500 : 1.1702178631207818 max of weights 0.39694175201448934\n",
      "Iteration 510 loss 0.7481412182103294 train RMSE 0.9088834066573348 Validation RMSE 510 : 1.1822298420809727 max of weights 0.40301710034540006\n",
      "Iteration 520 loss 0.7480625337880671 train RMSE 0.9087506818640463 Validation RMSE 520 : 1.1861601833367656 max of weights 0.42129813266317384\n",
      "Iteration 530 loss 0.7474598660366313 train RMSE 0.9079976040096555 Validation RMSE 530 : 1.1927145670512895 max of weights 0.4325896045534732\n",
      "Iteration 540 loss 0.7477679620423259 train RMSE 0.9083630816704847 Validation RMSE 540 : 1.1945429888852388 max of weights 0.4294378639532898\n",
      "Iteration 550 loss 0.7425464683169674 train RMSE 0.9019904355994678 Validation RMSE 550 : 1.1855272277915414 max of weights 0.436614487474077\n",
      "Iteration 560 loss 0.7433308657289484 train RMSE 0.9029144667044722 Validation RMSE 560 : 1.1793551632914834 max of weights 0.43682758517566195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 570 loss 0.746800384653625 train RMSE 0.9071060356634002 Validation RMSE 570 : 1.1717394464558863 max of weights 0.43554205152874464\n",
      "Iteration 580 loss 0.7461619737608755 train RMSE 0.9063075474934804 Validation RMSE 580 : 1.1632729431326834 max of weights 0.4295428789210161\n",
      "Iteration 590 loss 0.734579441449931 train RMSE 0.8921887275450894 Validation RMSE 590 : 1.1672547126098989 max of weights 0.43199149606044523\n",
      "Iteration 600 loss 0.7369490589954658 train RMSE 0.8950406350463174 Validation RMSE 600 : 1.1835877038304337 max of weights 0.4325035724910027\n",
      "Iteration 610 loss 0.7380291860466626 train RMSE 0.8963193608339377 Validation RMSE 610 : 1.1907861927036356 max of weights 0.4424381897056064\n",
      "Iteration 620 loss 0.7364815887260585 train RMSE 0.8944016720941563 Validation RMSE 620 : 1.1960273893620104 max of weights 0.4580562430347485\n",
      "Iteration 630 loss 0.7411658312135444 train RMSE 0.90008177643049 Validation RMSE 630 : 1.2054543627927945 max of weights 0.4612045074572156\n",
      "Iteration 640 loss 0.7358412344432644 train RMSE 0.8935944934814222 Validation RMSE 640 : 1.1975142717574436 max of weights 0.46033615448843607\n",
      "Iteration 650 loss 0.7343878824951945 train RMSE 0.8917988709083361 Validation RMSE 650 : 1.1945712372450685 max of weights 0.46750802742590897\n",
      "Iteration 660 loss 0.7380052725874429 train RMSE 0.8961641812538501 Validation RMSE 660 : 1.1855633801961492 max of weights 0.4629804422824597\n",
      "Iteration 670 loss 0.7367325747220612 train RMSE 0.894591577821051 Validation RMSE 670 : 1.176032818229859 max of weights 0.46153749288459456\n",
      "Iteration 680 loss 0.7373386847995451 train RMSE 0.8953041592827788 Validation RMSE 680 : 1.1693455876863212 max of weights 0.4561282874030541\n",
      "Iteration 690 loss 0.725342086020463 train RMSE 0.8806795578240758 Validation RMSE 690 : 1.1780130073092125 max of weights 0.46053931445577173\n",
      "Iteration 700 loss 0.7309611108263445 train RMSE 0.8874825857885574 Validation RMSE 700 : 1.198468625908791 max of weights 0.4604693413183433\n",
      "Iteration 710 loss 0.7290838042495588 train RMSE 0.8851583300884979 Validation RMSE 710 : 1.2036920782474942 max of weights 0.47164710434401363\n",
      "Iteration 720 loss 0.7287498680690271 train RMSE 0.8847220168700461 Validation RMSE 720 : 1.2087554775084566 max of weights 0.48442700498356606\n",
      "Iteration 730 loss 0.7348726441708583 train RMSE 0.8921540319059906 Validation RMSE 730 : 1.2166807892135492 max of weights 0.4818304026668701\n",
      "Iteration 740 loss 0.7260962775106726 train RMSE 0.8814612167649245 Validation RMSE 740 : 1.2037568093387827 max of weights 0.4854707025934794\n",
      "Iteration 750 loss 0.7250368352061922 train RMSE 0.8801428393123758 Validation RMSE 750 : 1.2055157986508327 max of weights 0.48918435146665895\n",
      "Iteration 760 loss 0.7278186494469797 train RMSE 0.8834895214192066 Validation RMSE 760 : 1.1941693474531516 max of weights 0.4848026195891819\n",
      "Iteration 770 loss 0.7232083499367427 train RMSE 0.877858302795832 Validation RMSE 770 : 1.1796906046074025 max of weights 0.4815116703443015\n",
      "Iteration 780 loss 0.718861045246595 train RMSE 0.8725407114839498 Validation RMSE 780 : 1.1770249388989193 max of weights 0.4795519159810987\n",
      "Iteration 790 loss 0.7148216108583779 train RMSE 0.8675936355544307 Validation RMSE 790 : 1.1927567628043558 \n",
      "Performance (RMSE) on activity:\n",
      "Train: 1.1064333601329057\n",
      "Test:  1.2115371533064665\n",
      "--------------------------------------------------------------------------------\n",
      "Starting neural fingerprint experiment...\n",
      "Total number of weights in the network: 41771\n",
      "max of weights 0.08535001578936458\n",
      "Iteration 0 loss 0.8187127378654213 train RMSE 0.9956669173433814 Validation RMSE 0 : 1.20552164057607 max of weights 0.07438252802890713\n",
      "Iteration 10 loss 0.8160962391088001 train RMSE 0.9925201814642607 Validation RMSE 10 : 1.2065253097589592 max of weights 0.0729394493817034\n",
      "Iteration 20 loss 0.815652419516367 train RMSE 0.9919839752081421 Validation RMSE 20 : 1.2084289317500778 max of weights 0.07753474261385916\n",
      "Iteration 30 loss 0.8151471304326946 train RMSE 0.9913712625474116 Validation RMSE 30 : 1.2083112679643948 max of weights 0.08506251631780189\n",
      "Iteration 40 loss 0.8148917593464117 train RMSE 0.9910563908448425 Validation RMSE 40 : 1.2094255061680563 max of weights 0.10069228476059333\n",
      "Iteration 50 loss 0.8153686514776366 train RMSE 0.9916278420309987 Validation RMSE 50 : 1.208085757198256 max of weights 0.11415551470497781\n",
      "Iteration 60 loss 0.8154100372116793 train RMSE 0.9916700463512872 Validation RMSE 60 : 1.2074971522620237 max of weights 0.1374732996292297\n",
      "Iteration 70 loss 0.8148510059345858 train RMSE 0.9909636627155354 Validation RMSE 70 : 1.206318136698028 max of weights 0.17244630793648913\n",
      "Iteration 80 loss 0.8191444279388908 train RMSE 0.9961067624808756 Validation RMSE 80 : 1.2019327341670716 max of weights 0.1956812342139452\n",
      "Iteration 90 loss 0.8055266503536748 train RMSE 0.9794195558929245 Validation RMSE 90 : 1.1825248772665766 max of weights 0.21417258623234356\n",
      "Iteration 100 loss 0.7724749763534866 train RMSE 0.9390636865370258 Validation RMSE 100 : 1.1067912234629569 max of weights 0.23663834429047118\n",
      "Iteration 110 loss 0.7549944477317218 train RMSE 0.9178146982330488 Validation RMSE 110 : 1.0419093395264392 max of weights 0.2555747613740551\n",
      "Iteration 120 loss 0.7577914194607244 train RMSE 0.9212006792494741 Validation RMSE 120 : 1.074062571727418 max of weights 0.2804931885999734\n",
      "Iteration 130 loss 0.7669644848912796 train RMSE 0.932309602916715 Validation RMSE 130 : 1.0072740162721678 max of weights 0.3039834030363612\n",
      "Iteration 140 loss 0.7428238576860257 train RMSE 0.90291905917691 Validation RMSE 140 : 1.0752646313556957 max of weights 0.3277287370664483\n",
      "Iteration 150 loss 0.7539629304659138 train RMSE 0.9164161478770931 Validation RMSE 150 : 1.0846920156655413 max of weights 0.3399168656802873\n",
      "Iteration 160 loss 0.7426799662380621 train RMSE 0.9026393784534212 Validation RMSE 160 : 1.0228537107903315 max of weights 0.3506264541914399\n",
      "Iteration 170 loss 0.7475243321882146 train RMSE 0.9084928631769079 Validation RMSE 170 : 1.0496795508336854 max of weights 0.3695622688292824\n",
      "Iteration 180 loss 0.7517022530444704 train RMSE 0.9135017892900509 Validation RMSE 180 : 1.044765477034151 max of weights 0.38142356364297064\n",
      "Iteration 190 loss 0.7374179030361194 train RMSE 0.8960739842127072 Validation RMSE 190 : 1.016023209625846 max of weights 0.3787485236097521\n",
      "Iteration 200 loss 0.7345279523646018 train RMSE 0.8925388442041274 Validation RMSE 200 : 1.0016948037821138 max of weights 0.37269003650474725\n",
      "Iteration 210 loss 0.7357709888928661 train RMSE 0.8940489463345156 Validation RMSE 210 : 1.057692770603686 max of weights 0.3573635968733659\n",
      "Iteration 220 loss 0.7338509830626003 train RMSE 0.8917224756086681 Validation RMSE 220 : 1.0469337939169294 max of weights 0.3598052232985222\n",
      "Iteration 230 loss 0.749145247590814 train RMSE 0.9103088370143994 Validation RMSE 230 : 1.0382126679211765 max of weights 0.3626185430624827\n",
      "Iteration 240 loss 0.74591320771378 train RMSE 0.9063350969081383 Validation RMSE 240 : 1.0789404889621854 max of weights 0.3587821913418182\n",
      "Iteration 250 loss 0.7331536653085687 train RMSE 0.8908349995337964 Validation RMSE 250 : 1.0699458763486829 max of weights 0.366440113002097\n",
      "Iteration 260 loss 0.731273939928097 train RMSE 0.8885402644511138 Validation RMSE 260 : 1.0525414448630537 max of weights 0.38330161978524563\n",
      "Iteration 270 loss 0.773647580769109 train RMSE 0.9400517981438793 Validation RMSE 270 : 1.0564441684375903 max of weights 0.39817174398169375\n",
      "Iteration 280 loss 0.7394445490846921 train RMSE 0.8984521658146948 Validation RMSE 280 : 1.0425951989531608 max of weights 0.4087411990664052\n",
      "Iteration 290 loss 0.7318260840397138 train RMSE 0.8891797532275566 Validation RMSE 290 : 1.0365422280266159 max of weights 0.40004621570083915\n",
      "Iteration 300 loss 0.7299486315042952 train RMSE 0.8868839405523027 Validation RMSE 300 : 1.0113992136449963 max of weights 0.3891761299145503\n",
      "Iteration 310 loss 0.7333344721679638 train RMSE 0.8910015870868735 Validation RMSE 310 : 1.0411399767025542 max of weights 0.41113493072061574\n",
      "Iteration 320 loss 0.7447949597671223 train RMSE 0.9049411631096678 Validation RMSE 320 : 1.0292245563761195 max of weights 0.4309480429590186\n",
      "Iteration 330 loss 0.7715932176886365 train RMSE 0.9375129890101243 Validation RMSE 330 : 1.0606358876229067 max of weights 0.46036885711977255\n",
      "Iteration 340 loss 0.7480584940998658 train RMSE 0.9088690964671936 Validation RMSE 340 : 1.073659900476251 max of weights 0.4735520971116654\n",
      "Iteration 350 loss 0.7415304965503685 train RMSE 0.9009411488895465 Validation RMSE 350 : 1.042273308974437 max of weights 0.47408110666281794\n",
      "Iteration 360 loss 0.7428365133771975 train RMSE 0.9025262449981379 Validation RMSE 360 : 1.0693558214762344 max of weights 0.46973800470254906\n",
      "Iteration 370 loss 0.7838363003891164 train RMSE 0.9523573079985083 Validation RMSE 370 : 1.0356532214301215 max of weights 0.47109141759141976\n",
      "Iteration 380 loss 0.7312650182067602 train RMSE 0.8884156415085878 Validation RMSE 380 : 1.0343376423651705 max of weights 0.4814897167649139\n",
      "Iteration 390 loss 0.7345038591528276 train RMSE 0.8923405513503159 Validation RMSE 390 : 1.045072093569691 max of weights 0.4983925132906083\n",
      "Iteration 400 loss 0.7331252506693651 train RMSE 0.8906555690768376 Validation RMSE 400 : 1.0304139778633579 max of weights 0.513291486521831\n",
      "Iteration 410 loss 0.7393144059580466 train RMSE 0.8981776618782595 Validation RMSE 410 : 1.050949834449619 max of weights 0.5363915661382848\n",
      "Iteration 420 loss 0.7609167820120025 train RMSE 0.9244596200368916 Validation RMSE 420 : 1.02596299767079 max of weights 0.5567475569485802\n",
      "Iteration 430 loss 0.7711367337256277 train RMSE 0.9368602908039074 Validation RMSE 430 : 1.0458662260658542 max of weights 0.587327083479558\n",
      "Iteration 440 loss 0.7443321202215377 train RMSE 0.9042511287995206 Validation RMSE 440 : 1.046864271836401 max of weights 0.596243957478997\n",
      "Iteration 450 loss 0.7551173797875083 train RMSE 0.9173694998812542 Validation RMSE 450 : 1.0360712793751525 max of weights 0.590140898500594\n",
      "Iteration 460 loss 0.771828875003141 train RMSE 0.9376945796129917 Validation RMSE 460 : 1.0836991686041615 max of weights 0.583739639452886\n",
      "Iteration 470 loss 0.7697649172522419 train RMSE 0.9351648713653425 Validation RMSE 470 : 1.021298265590641 max of weights 0.5867972044241982\n",
      "Iteration 480 loss 0.7332743844134302 train RMSE 0.8907783907901281 Validation RMSE 480 : 1.0311488821506625 max of weights 0.5976177457998141\n",
      "Iteration 490 loss 0.7407840745878235 train RMSE 0.8998920276690578 Validation RMSE 490 : 1.0284952675968195 max of weights 0.6134584939085298\n",
      "Iteration 500 loss 0.7448818791688062 train RMSE 0.9048670975329945 Validation RMSE 500 : 1.0418440204251165 max of weights 0.6264744653423623\n",
      "Iteration 510 loss 0.7542466189246675 train RMSE 0.9162515881782336 Validation RMSE 510 : 1.0426154513506232 max of weights 0.6425618214949752\n",
      "Iteration 520 loss 0.7672290871908426 train RMSE 0.9320460294158017 Validation RMSE 520 : 1.0276414622929497 max of weights 0.6626458545546117\n",
      "Iteration 530 loss 0.7652045483732012 train RMSE 0.9295498936338683 Validation RMSE 530 : 1.0404030344529178 max of weights 0.684169237539251\n",
      "Iteration 540 loss 0.7590017125538434 train RMSE 0.9220111364613587 Validation RMSE 540 : 1.0323371327250837 max of weights 0.6833665926786777\n",
      "Iteration 550 loss 0.7842966073314797 train RMSE 0.9527692417724204 Validation RMSE 550 : 1.0275434458470143 max of weights 0.6763836869608676\n",
      "Iteration 560 loss 0.7853168442709713 train RMSE 0.954005780245009 Validation RMSE 560 : 1.0456857361616934 max of weights 0.6712329881638819\n",
      "Iteration 570 loss 0.7527061889067028 train RMSE 0.9143331923258904 Validation RMSE 570 : 1.0052514662885814 max of weights 0.6734677378312026\n",
      "Iteration 580 loss 0.7413384132872121 train RMSE 0.9004846407538187 Validation RMSE 580 : 1.0206834259232513 max of weights 0.6850321682887021\n",
      "Iteration 590 loss 0.7448763120606594 train RMSE 0.9047628291945121 Validation RMSE 590 : 1.0135108834598214 max of weights 0.6959057845606573\n",
      "Iteration 600 loss 0.7513463767098557 train RMSE 0.9126229287377792 Validation RMSE 600 : 1.052179052885087 max of weights 0.7090725303824328\n",
      "Iteration 610 loss 0.7584489380795324 train RMSE 0.9212696756931523 Validation RMSE 610 : 1.0270818631642775 max of weights 0.7192683607047061\n",
      "Iteration 620 loss 0.7640650972374164 train RMSE 0.928089973673023 Validation RMSE 620 : 1.0090551394278238 max of weights 0.7367281013935237\n",
      "Iteration 630 loss 0.7613420988556617 train RMSE 0.9247537360814504 Validation RMSE 630 : 1.033008448503697 max of weights 0.7466144563644531\n",
      "Iteration 640 loss 0.7689288146541509 train RMSE 0.9339874101581221 Validation RMSE 640 : 1.0222655896317479 max of weights 0.7395590329525704\n",
      "Iteration 650 loss 0.7733946566748333 train RMSE 0.9394123711140704 Validation RMSE 650 : 1.0162771735848677 max of weights 0.7301795599636438\n",
      "Iteration 660 loss 0.8078627457629501 train RMSE 0.9813191131994516 Validation RMSE 660 : 1.001494921705559 max of weights 0.7238611924888955\n",
      "Iteration 670 loss 0.7482718347231111 train RMSE 0.9088410871683408 Validation RMSE 670 : 0.9864220606298008 max of weights 0.7255077660564744\n",
      "Iteration 680 loss 0.7502146589532404 train RMSE 0.9111772498088218 Validation RMSE 680 : 1.0124819817122046 max of weights 0.7363801733731306\n",
      "Iteration 690 loss 0.7435301685481855 train RMSE 0.9030226343050275 Validation RMSE 690 : 1.005674161855512 max of weights 0.7451192576118878\n",
      "Iteration 700 loss 0.7453644604353998 train RMSE 0.9052487425327421 Validation RMSE 700 : 1.0301588349784 max of weights 0.7564307364946719\n",
      "Iteration 710 loss 0.7600934313049099 train RMSE 0.9231779168718887 Validation RMSE 710 : 1.001247668414376 max of weights 0.7647501542654874\n",
      "Iteration 720 loss 0.7724000929662924 train RMSE 0.938125711566082 Validation RMSE 720 : 0.9954733308973925 max of weights 0.7835296294773871\n",
      "Iteration 730 loss 0.7632353160857417 train RMSE 0.9269688657009596 Validation RMSE 730 : 0.9987570217554592 max of weights 0.7867268449839601\n",
      "Iteration 740 loss 0.7734709286949871 train RMSE 0.9394177188040291 Validation RMSE 740 : 1.009979012395553 max of weights 0.7742518103936639\n",
      "Iteration 750 loss 0.7757929540462131 train RMSE 0.9422335284434655 Validation RMSE 750 : 1.0162001485113814 max of weights 0.7633117250361486\n",
      "Iteration 760 loss 0.7932358220909813 train RMSE 0.9634251555247225 Validation RMSE 760 : 0.9860523511444149 max of weights 0.7583887683629076\n",
      "Iteration 770 loss 0.7499919906837491 train RMSE 0.9108288885855096 Validation RMSE 770 : 0.9823310512557435 max of weights 0.7603860625892938\n",
      "Iteration 780 loss 0.7415320148944491 train RMSE 0.900519485488387 Validation RMSE 780 : 0.9951260561671911 max of weights 0.7689349653766312\n",
      "Iteration 790 loss 0.7373187007245106 train RMSE 0.8953742077714848 Validation RMSE 790 : 1.0073957735476633\n",
      "Morgan test RMSE: 1.2115371533064665 Neural test RMSE: 1.0534474622904428\n"
     ]
    }
   ],
   "source": [
    "# Example regression script using neural fingerprints.\n",
    "#\n",
    "# Compares Morgan fingerprints to neural fingerprints.\n",
    "\n",
    "import autograd.numpy as np\n",
    "import autograd.numpy.random as npr\n",
    "\n",
    "from neuralfingerprint import load_data\n",
    "from neuralfingerprint import build_morgan_deep_net\n",
    "from neuralfingerprint import build_conv_deep_net\n",
    "from neuralfingerprint import normalize_array, adam\n",
    "from neuralfingerprint import build_batched_grad\n",
    "from neuralfingerprint.util import rmse\n",
    "\n",
    "from autograd import grad\n",
    "\n",
    "task_params = {'target_name' : 'activity',\n",
    "               'data_file'   : 'malaria-processed.csv'}\n",
    "N_train = 9700\n",
    "N_val   = 100\n",
    "N_test  = 100\n",
    "\n",
    "model_params = dict(fp_length=50,    # Usually neural fps need far fewer dimensions than morgan.\n",
    "                    fp_depth=6,      # The depth of the network equals the fingerprint radius.\n",
    "                    conv_width=20,   # Only the neural fps need this parameter.\n",
    "                    h1_size=100,     # Size of hidden layer of network on top of fps.\n",
    "                    L2_reg=np.exp(-2))\n",
    "train_params = dict(num_iters=800,\n",
    "                    batch_size=100,\n",
    "                    init_scale=np.exp(-4),\n",
    "                    step_size=np.exp(-6))\n",
    "\n",
    "# Define the architecture of the network that sits on top of the fingerprints.\n",
    "vanilla_net_params = dict(\n",
    "    layer_sizes = [model_params['fp_length'], model_params['h1_size']],  # One hidden layer.\n",
    "    normalize=True, L2_reg = model_params['L2_reg'], nll_func = rmse)\n",
    "\n",
    "def train_nn(pred_fun, loss_fun, num_weights, train_smiles, train_raw_targets, train_params, seed=0,\n",
    "             validation_smiles=None, validation_raw_targets=None):\n",
    "    \"\"\"loss_fun has inputs (weights, smiles, targets)\"\"\"\n",
    "    print \"Total number of weights in the network:\", num_weights\n",
    "    init_weights = npr.RandomState(seed).randn(num_weights) * train_params['init_scale']\n",
    "\n",
    "    num_print_examples = 100\n",
    "    train_targets, undo_norm = normalize_array(train_raw_targets)\n",
    "    training_curve = []\n",
    "    def callback(weights, iter):\n",
    "        if iter % 10 == 0:\n",
    "            print \"max of weights\", np.max(np.abs(weights))\n",
    "            train_preds = undo_norm(pred_fun(weights, train_smiles[:num_print_examples]))\n",
    "            cur_loss = loss_fun(weights, train_smiles[:num_print_examples], train_targets[:num_print_examples])\n",
    "            training_curve.append(cur_loss)\n",
    "            print \"Iteration\", iter, \"loss\", cur_loss,\\\n",
    "                  \"train RMSE\", rmse(train_preds, train_raw_targets[:num_print_examples]),\n",
    "            if validation_smiles is not None:\n",
    "                validation_preds = undo_norm(pred_fun(weights, validation_smiles))\n",
    "                print \"Validation RMSE\", iter, \":\", rmse(validation_preds, validation_raw_targets),\n",
    "\n",
    "    # Build gradient using autograd.\n",
    "    grad_fun = grad(loss_fun)\n",
    "    grad_fun_with_data = build_batched_grad(grad_fun, train_params['batch_size'],\n",
    "                                            train_smiles, train_targets)\n",
    "\n",
    "    # Optimize weights.\n",
    "    trained_weights = adam(grad_fun_with_data, init_weights, callback=callback,\n",
    "                           num_iters=train_params['num_iters'], step_size=train_params['step_size'])\n",
    "\n",
    "    def predict_func(new_smiles):\n",
    "        \"\"\"Returns to the original units that the raw targets were in.\"\"\"\n",
    "        return undo_norm(pred_fun(trained_weights, new_smiles))\n",
    "    return predict_func, trained_weights, training_curve\n",
    "\n",
    "\n",
    "def main():\n",
    "    print \"Loading data...\"\n",
    "    traindata, valdata, testdata = load_data(\n",
    "        task_params['data_file'], (N_train, N_val, N_test),\n",
    "        input_name='smiles', target_name=task_params['target_name'])\n",
    "    train_inputs, train_targets = traindata\n",
    "    val_inputs,   val_targets   = valdata\n",
    "    test_inputs,  test_targets  = testdata\n",
    "\n",
    "    def print_performance(pred_func):\n",
    "        train_preds = pred_func(train_inputs)\n",
    "        val_preds = pred_func(val_inputs)\n",
    "        print \"\\nPerformance (RMSE) on \" + task_params['target_name'] + \":\"\n",
    "        print \"Train:\", rmse(train_preds, train_targets)\n",
    "        print \"Test: \", rmse(val_preds,  val_targets)\n",
    "        print \"-\" * 80\n",
    "        return rmse(val_preds, val_targets)\n",
    "\n",
    "    def run_morgan_experiment():\n",
    "        loss_fun, pred_fun, net_parser = \\\n",
    "            build_morgan_deep_net(model_params['fp_length'],\n",
    "                                  model_params['fp_depth'], vanilla_net_params)\n",
    "        num_weights = len(net_parser)\n",
    "        predict_func, trained_weights, conv_training_curve = \\\n",
    "            train_nn(pred_fun, loss_fun, num_weights, train_inputs, train_targets,\n",
    "                     train_params, validation_smiles=val_inputs, validation_raw_targets=val_targets)\n",
    "        return print_performance(predict_func)\n",
    "\n",
    "    def run_conv_experiment():\n",
    "        conv_layer_sizes = [model_params['conv_width']] * model_params['fp_depth']\n",
    "        conv_arch_params = {'num_hidden_features' : conv_layer_sizes,\n",
    "                            'fp_length' : model_params['fp_length'], 'normalize' : 1}\n",
    "        loss_fun, pred_fun, conv_parser = \\\n",
    "            build_conv_deep_net(conv_arch_params, vanilla_net_params, model_params['L2_reg'])\n",
    "        num_weights = len(conv_parser)\n",
    "        predict_func, trained_weights, conv_training_curve = \\\n",
    "            train_nn(pred_fun, loss_fun, num_weights, train_inputs, train_targets,\n",
    "                     train_params, validation_smiles=val_inputs, validation_raw_targets=val_targets)\n",
    "        test_predictions = predict_func(test_inputs)\n",
    "        return rmse(test_predictions, test_targets)\n",
    "\n",
    "    print \"Task params\", task_params\n",
    "    print\n",
    "    print \"Starting Morgan fingerprint experiment...\"\n",
    "    test_loss_morgan = run_morgan_experiment()\n",
    "    print \"Starting neural fingerprint experiment...\"\n",
    "    test_loss_neural = run_conv_experiment()\n",
    "    print\n",
    "    print \"Morgan test RMSE:\", test_loss_morgan, \"Neural test RMSE:\", test_loss_neural\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
